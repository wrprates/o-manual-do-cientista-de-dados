[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "O manual do Cientista de Dados",
    "section": "",
    "text": "Prefácio\nSeja bem-vindo(a) ao “O Manual do Cientista de Dados”. Este projeto é uma jornada contínua para criar um recurso acessível e gratuito para todos que desejam explorar o fascinante mundo da ciência de dados.\nSou Wlademir Ribeiro Prates, Cientista de Dados Senior e PhD em Finanças pela UFSC. Ao longo da minha carreira, tive a oportunidade de trabalhar em diversas empresas, tanto em consultoria quanto integrando times de dados em grandes organizações, no Brasil e no exterior. Essa experiência me motivou a compartilhar conhecimento e contribuir para o crescimento da comunidade de ciência de dados.\nEste livro é um esforço colaborativo, e sua contribuição é essencial. Sinta-se à vontade para participar, seja através de sugestões, correções ou novas ideias. Juntos, podemos construir um material de alta qualidade que beneficie a todos.\nAcompanhe o desenvolvimento do livro e acesse a versão mais recente em O Manual do Cientista de Dados. Vamos nos conectar e trocar experiências nas redes sociais: LinkedIn e Instagram.\nAgradeço seu interesse e apoio. Espero que este livro seja uma fonte valiosa de aprendizado e inspiração para sua jornada na ciência de dados.",
    "crumbs": [
      "Prefácio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdução",
    "section": "",
    "text": "A ciência de dados está rapidamente se tornando uma das áreas mais dinâmicas e promissoras do mercado de trabalho global. Com a crescente digitalização e a explosão de dados gerados diariamente, a demanda por profissionais capazes de transformar dados brutos em insights valiosos nunca foi tão alta. No Brasil, os salários para cientistas de dados são, em média, três vezes maiores do que a média nacional, refletindo a importância e a escassez de profissionais qualificados na área.\nAlém disso, a ciência de dados oferece oportunidades únicas de internacionalização de carreira. Com a possibilidade de trabalhar remotamente, muitos profissionais têm a chance de colaborar com empresas ao redor do mundo, sem sair de casa. Essa flexibilidade não só amplia as oportunidades de emprego, mas também permite que os cientistas de dados se conectem com uma rede global de profissionais e projetos inovadores.\nEste livro foi criado para ser um guia prático e acessível, focado em ajudar você a se desenvolver como um profissional de ciência de dados. Ao longo das páginas, você encontrará conceitos fundamentais, técnicas avançadas e exemplos práticos que irão equipá-lo com as habilidades necessárias para prosperar neste campo em constante evolução.\nPrepare-se para embarcar em uma jornada de aprendizado que não apenas ampliará seu conhecimento técnico, mas também abrirá portas para um futuro repleto de possibilidades emocionantes na ciência de dados. Vamos juntos explorar este universo e transformar dados em oportunidades.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html",
    "href": "o_que_e_ds.html",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "",
    "text": "2.1 O que é ciência de dados (data science)?\nPara auxiliar na definição do termo, vamos ver o que alguns autores e sites especializados dizem a respeito de ciência de dados:\nDefinição que acredito:\nA forma como cada uma das subáreas contribui para o campo de ciência de dados é, de maneira geral, a seguinte:\nUm dos pontos principais para um bom funcionamento de um projeto de ciência de dados na prática é um perfeito alinhamento entre a equipe técnica (os cientistas de dados) e a área de negócio do cliente (interno ou externo). É comum que sejam realizadas sessões de Design Thinking e utilizadas adaptações do Business Canvas para identificar o problema de negócio e também gerar insights relevantes à equipe técnica, o que deverá conduzir a entregas de melhor qualidade.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#o-que-é-ciência-de-dados-data-science",
    "href": "o_que_e_ds.html#o-que-é-ciência-de-dados-data-science",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "",
    "text": "“A ciência de dados é uma disciplina multifacetada, que abrange [aprendizado de máquina] e outros processos analíticos, estatísticas e ramos relacionados da matemática. Cada vez mais se utiliza da computação científica de alto desempenho, tudo isso para extrair informações dos dados e usar essas informações encontradas para contar histórias.” (Matthew Mayo, KDnuggets).\n“Data science envolve princípios, processos e técnicas para compreender fenômenos por meio da análise (automatizada) de dados.” (Provost, Fawcett, and Boscato 2016, 4).\n“A ciência de dados é um conjunto multidisciplinar de inferência de dados, desenvolvimento de algoritmos e tecnologia para resolver problemas analiticamente complexos. No centro estão os dados: um grande número de informações brutas, transmitidas e armazenadas em data warehouses corporativos. […] A ciência de dados é basicamente o uso desses dados de maneiras criativas para gerar valor aos negócios.” (Divya Singh, Data Science Central).\n\n\n\nCiência de dados é uma área multidisciplinar que se utiliza principalmente, mas não apenas, de método científico, estatística, conhecimento de negócio e ciência da computação para gerar valor aos negócios.\n\n\n\nMétodo científico: auxilia na estrutura do projeto de data science, que contempla a definição de um problema de negócio (análogo ao problema de pesquisa dos trabalhos acadêmicos); definição de objetivos geral e específicos; discussão e apresentação de resultados; conclusões e procedimentos futuros.\nEstatística: formas de resumir e visualizar dados; testes de hipóteses; técnicas de análise preditiva.\nNegócio: necessário para ser capaz de definir hipóteses de negócio a serem posteriormente transformadas em hipóteses estatísticas; fundamental para escolher o problema de negócio de fato mais relevante para se investir tempo e dinheiro com análise de dados.\nCiência da computação: contribui com métodos que impulsionam as técnicas estatíticas (machine learning), utilizando poder computacional, linguagens de programação, computação na nuvem, bancos de dados, entre outros.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#surgimento-da-ciência-de-dados",
    "href": "o_que_e_ds.html#surgimento-da-ciência-de-dados",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.2 Surgimento da ciência de dados",
    "text": "2.2 Surgimento da ciência de dados\nA área de ciência de dados é bastante nova, tendo sido assim chamada pela primeira vez a princípio em 2001. Porém, foi a partir de aproximadamente o ano de 2010 que a área começou a ganhar notoriedade, principalmente devido à onda de big data.\nA razão para as empresas passarem a gerar e armazenar grandes volumes de dados (big data) se deu com o avanço da tecnologia a partir da bolha “ponto com”, e principalmente com o barateamento dos discos rígidos.\nA partir disso, grandes companhias como Google e Amazon desenvolveram novas arquiteturas computacionais, que chamamos hoje de computação nas nuvens (cloud computing).\nCom todo este cenário ficou fácil e barato para as empresas armazenarem diversos tipos de dados, muitos que até então eram ignorados.\nA grande pergunta que surgiu foi “o que fazer com todos estes dados, e como gerar valor de negócio a partir deles?”.\nSendo assim, o termo big data saiu um pouco de enfoque, dando lugar à multidisciplinariedade da ciência de dados, em que big data é apenas uma parte de um todo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#perfil-e-papel-do-cientista-de-dados",
    "href": "o_que_e_ds.html#perfil-e-papel-do-cientista-de-dados",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.3 Perfil e papel do cientista de dados",
    "text": "2.3 Perfil e papel do cientista de dados\nO objetivo aqui é de trazer uma base do perfil e também do papel do cientista de dados nas organizações. Claro que dependendo da companhia a opinião pode divergir em alguns aspectos. Por isso, meu objetivo aqui não é focar em habilidades técnicas exigidas, nem tampouco em linguagens de programação ou tecnologias.\nInicialmente, apresento uma citação de Provost, Fawcett, and Boscato (2016, 333) que resume muito bem o processo de ciência de dados no ponto de vista do cientista de dados:\n\nA prática de data science pode ser melhor descrita como uma combinação de engenharia analítica e exploração. O negócio apresenta um problema que gostaríamos de resolver. Raramente, o problema de negócio é, de modo direto, uma de nossas tarefas básicas de mineração de dados. Decompomos o problema em subtarefas que achamos que podemos resolver, geralmente, começando com as ferramentas existentes. Para algumas dessas tarefas podemos não saber o quão bem podemos resolvê-las, por isso, temos que explorar os dados e fazer uma avaliação para verificar. Se isso não for possível, poderemos ter de tentar algo completamente diferente. No processo, podemos descobrir o conhecimento que vai nos ajudar a resolver o problema que queremos ou podemos descobrir algo inesperado que nos leva a outros sucessos importantes.\n\nEste parágrafo acima é excelente, pois resume algumas das principais capacidades que um cientista de dados deve ter:\n\nCriatividade.\nCapacidade de explorar possibilidades de soluções de problemas até então desconhecidas.\nPensamento crítico para ser capaz de extrair conclusões importantes, resultantes dos processos de análise de dados, mas que não faziam parte diretamente da questão inicial levantada. Claro que sobre este ponto vale ressaltar que o cientista de dados precisa ter cuidado para não perder o foco do projeto. É preciso ter bom senso.\n\nUm dos pontos que me chama atenção no perfil do profissional cientista de dados é que ter conhecimento do método científico contribui muito para a execução prática dos projetos de ciência de dados. Isto faz com que a área de data science seja capaz de interligar a “teoria” do mundo acadêmico com a “prática” do mundo dos negócios.\nNa minha opinião, sempre achei que a academia e o meio corporativo têm muito a aprender um com o outro. O campo de ciência de dados é um exemplo de sucesso desta mescla de abordagens.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#ferramentas-do-cientista-de-dados",
    "href": "o_que_e_ds.html#ferramentas-do-cientista-de-dados",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.4 Ferramentas do cientista de dados",
    "text": "2.4 Ferramentas do cientista de dados\nNão há necessariamente um conjunto de ferramentas padrão para trabalhar com ciência de dados. Porém, devido à característica do trabalho, alguns pontos importantes são:\n\nTer conhecimento de alguma linguagem de programação com alto poder de aplicação analítica. Hoje em dia se destacam as linguagens R e Python.\nTer conhecimentos intermediários em métodos estatísticos.\nCapacidade didática de explicar os resultados encontrados, principalmente de forma escrita.\nTer conhecimentos básicos de computação na nuvem.\nSaber trabalhar com versionamento de códigos (basicamente Git).\nOutros conhecimentos são importantes, mas que talvez não sejam tão essenciais quanto os demais, que são: bancos de dados, html, javascript.\n\nComo já mencionado, o trabalho do cientista de dados é muito versátil e dinâmico. Por isso, quanto mais conhecimentos o indivíduo tiver melhor, mas as linguagens R e Python são tão poderosas para fins analíticos e tão integradoras de outras tecnologias que geralmente não é necessário ter profundos conhecimentos além delas para executar bons projetos de ciência de dados.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#aplicações-ciência-de-dados-para-negócios",
    "href": "o_que_e_ds.html#aplicações-ciência-de-dados-para-negócios",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.5 Aplicações: ciência de dados para negócios",
    "text": "2.5 Aplicações: ciência de dados para negócios\nAs aplicações são diversas, mas a seguir tento exemplificar algumas possibilidades de aplicação de ciência de dados na solução prática de problemas de negócio reais:\n\n2.5.1 Recursos humanos\n\nTurnover: identificação das causas principais que levam um colaborador a pedir demissão da empresa, bem como aplicação de modelo preditivo para gerar uma lista com os colaboradores com maior probabilidade de pedirem para sair.\nRecrutamento: identificar os perfis de candidatos (internos ou externos) a vagas dentro da empresa que melhor se enquadram, utilizando dados de perfil, CV, experiências anteriores, aspectos demográficos, entre outros.\n\n\n\n2.5.2 Logística\n\nFalhas em entregas: identificação, por meio de modelo preditivo, dos casos em que há maior probabilidade de uma entrega não ser efetivida.\n\n\n\n2.5.3 Finanças\n\nGestão de carteiras: identificação dos ativos com maior probabilidade de apresentarem bom desempenho no futuro com base em diversos dados históricos. É possível mesclar dados contábeis/fundamentalistas com indicadores técnicos e também variáveis categóricas, como setor ou níveis de governança corporativa, por exemplo.\n\n\n\n2.5.4 Marketing\n\nIdentificação de leads: utilizar modelo preditivo para encontrar leads com maior probabilidade de se tornarem clientes.\nRedução de churn (clientes que cancelam assinaturas): abordagem muito próxima a utilizada no caso de turnover (colaboradores que pedem demissão).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#ciclo-de-vida-de-um-projeto-de-ciência-de-dados",
    "href": "o_que_e_ds.html#ciclo-de-vida-de-um-projeto-de-ciência-de-dados",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.6 Ciclo de Vida de um Projeto de Ciência de Dados",
    "text": "2.6 Ciclo de Vida de um Projeto de Ciência de Dados\nA ciência de dados se tornou um componente crucial em muitos setores, fornecendo insights valiosos e informando decisões baseadas em dados. Compreender as fases do ciclo de vida de um projeto de ciência de dados é essencial para obter os melhores resultados.\nUm projeto de ciência de dados é uma série de etapas inter-relacionadas. Cada etapa desempenha um papel importante na obtenção de insights significativos dos dados. Vamos entender melhor cada uma dessas fases.\n\n2.6.1 Definição do Problema\nA fase inicial de qualquer projeto de ciência de dados é a definição do problema. É aqui que identificamos a questão ou desafio que queremos resolver. Esta fase define o rumo do projeto, garantindo que as soluções geradas sejam relevantes e impactantes.\nUma clara definição do problema serve como guia para as fases subsequentes do projeto, incluindo a coleta e análise de dados. Garantir que o problema esteja bem definido desde o início é vital para o sucesso de qualquer projeto de ciência de dados.\n\n\n2.6.2 Coleta de Dados\nDepois de definir claramente o problema, o próximo passo é a coleta de dados. Os dados podem ser obtidos de diversas fontes, como bancos de dados internos, APIs da web ou fontes de terceiros. A escolha das fontes de dados depende da natureza do problema que estamos tentando resolver.\nOs dados coletados formam a base do projeto. Portanto, é essencial garantir que os dados sejam relevantes para o problema e de alta qualidade. Dados de baixa qualidade ou irrelevantes podem levar a insights imprecisos e soluções ineficazes.\n\n\n2.6.3 Preparação de Dados\nUma vez que os dados foram coletados, eles precisam ser preparados para análise. A preparação de dados inclui a limpeza dos dados, como tratar valores ausentes e remover outliers. Mas também abrange a transformação de dados e a criação de novas variáveis, que são partes fundamentais da preparação dos dados.\nEssa etapa é vital, pois a qualidade dos dados afeta a qualidade dos insights e soluções geradas. Sem um adequado preparo dos dados, corremos o risco de tirar conclusões erradas e propor soluções que não resolvam efetivamente o problema.\n\n\n2.6.4 Modelagem de Dados\nA modelagem de dados é onde aplicamos técnicas e algoritmos de aprendizado de máquina aos nossos dados preparados. A escolha do modelo a ser usado depende do problema que estamos tentando resolver. Podemos empregar desde modelos mais simples, como regressões, até abordagens mais complexas, como redes neurais.\nEsta fase é a essência da ciência de dados, onde os dados são transformados em insights valiosos. Um bom modelo pode extrair informações significativas dos dados, proporcionando soluções eficazes para o problema definido.\n\n\n2.6.5 Avaliação do Modelo de Dados\nA avaliação do modelo é a última fase do ciclo de vida de um projeto de ciência de dados. Aqui, testamos o desempenho do nosso modelo. Verificamos se o modelo é capaz de fornecer insights precisos e úteis para o problema.\nAs métricas de avaliação variam dependendo do problema e do tipo de modelo usado. O objetivo é garantir que o modelo seja não só preciso, mas também relevante e útil para resolver o problema que foi definido no início do projeto.\nCada projeto de ciência de dados é único e pode exigir abordagens diferentes. No entanto, as fases básicas descritas aqui proporcionam uma estrutura sólida que pode ser adaptada conforme necessário. Dominar essas fases será um trampolim para qualquer projeto de ciência de dados bem-sucedido.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#metodologia-ágil-em-ciência-de-dados",
    "href": "o_que_e_ds.html#metodologia-ágil-em-ciência-de-dados",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.7 Metodologia Ágil em Ciência de Dados",
    "text": "2.7 Metodologia Ágil em Ciência de Dados\nA aplicação de metodologias ágeis em projetos de ciência de dados tem se mostrado extremamente eficaz, especialmente considerando a natureza iterativa e experimental destes projetos. O Scrum, em particular, tem sido amplamente adotado, com algumas adaptações específicas para projetos de dados.\n\n2.7.1 Scrum em Projetos de Data Science\nO Scrum em ciência de dados mantém seus princípios fundamentais, mas adapta-se às particularidades da área:\n\nSprints: Geralmente de 2 a 4 semanas, com objetivos específicos como:\n\nSprint 1: Definição do problema e coleta inicial de dados\nSprint 2: Limpeza e preparação dos dados\nSprint 3: Desenvolvimento do primeiro modelo (MVP)\nSprints subsequentes: Iterações e melhorias do modelo\n\n\n\n\n2.7.2 Cerimônias Essenciais\n\nDaily Scrum: Reuniões diárias de 15 minutos onde a equipe discute:\n\nProgresso na preparação dos dados\nResultados preliminares dos modelos\nBloqueios técnicos encontrados\n\nSprint Planning: Define os objetivos da sprint, como:\n\nMétricas a serem alcançadas\nQuantidade de dados a ser processada\nFeatures a serem desenvolvidas\n\nSprint Review: Apresentação dos resultados para stakeholders:\n\nDemonstração dos modelos desenvolvidos\nApresentação de métricas alcançadas\nVisualizações de dados relevantes\n\nSprint Retrospective: Reflexão sobre o processo:\n\nO que funcionou bem no desenvolvimento dos modelos\nDesafios na coleta ou processamento de dados\nAjustes necessários para a próxima sprint\n\n\n\n\n2.7.3 Artefatos Adaptados\n\nProduct Backlog: Lista priorizada incluindo:\n\nFeatures do modelo a serem desenvolvidas\nConjuntos de dados a serem incorporados\nMétricas de performance a serem alcançadas\n\nSprint Backlog: Tarefas específicas como:\n\nLimpeza de determinado conjunto de dados\nImplementação de algoritmos específicos\nDesenvolvimento de visualizações\n\nQuadro Kanban: Adaptado com colunas como:\n\nColeta de Dados\nPreparação\nModelagem\nValidação\nProdução\n\n\n\n\n2.7.4 Papéis Principais\n\nProduct Owner: Foca em:\n\nDefinição clara dos objetivos de negócio\nPriorização de features do modelo\nValidação dos resultados do ponto de vista do negócio\n\nScrum Master: Auxilia removendo impedimentos como:\n\nAcesso a dados necessários\nRecursos computacionais adequados\nComunicação com áreas de negócio\n\nTime de Data Science: Composto por:\n\nCientistas de dados\nEngenheiros de dados\nAnalistas de negócio\n\n\nEsta estrutura ágil permite que projetos de ciência de dados mantenham o foco na entrega de valor, enquanto permanecem flexíveis para incorporar novos insights e requisitos que surgem durante o desenvolvimento dos modelos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#conclusões",
    "href": "o_que_e_ds.html#conclusões",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.8 Conclusões",
    "text": "2.8 Conclusões\nA área de ciência de dados muito se desenvolveu, e hoje as empresas em geral já vêem valor tanto em contratar projetos quanto em construir áreas de data science. A maior parte das empresas grandes, na verdade, já possuem áreas de ciência de dados constituídas. Contudo, ainda há muito que se consolidar em termos metodológicos e também de quais são os tipos de entregas mais adequadas.\nVale lembrar, tanto para gestores das áreas de ciência de dados, quanto para cientistas de dados, que o papel mais importante de um projeto de data science não está no modelo de machine learning utilizado, mas sim na capacidade de impactar positivamente a organização em algum KPI de negócio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "o_que_e_ds.html#referências",
    "href": "o_que_e_ds.html#referências",
    "title": "2  O que é Ciência de Dados (Data Science)?",
    "section": "2.9 Referências",
    "text": "2.9 Referências\nPROVOST, F., FAWCETT, T. (2016). Data science para negócios: o que você precisa saber sobre mineração de dados e pensamento analítico de dados. Rio de Janeiro: Alta Books.\n\n\n\n\nProvost, F., T. Fawcett, and M. Boscato. 2016. Data Science Para Negócios. ALTA BOOKS. https://books.google.com.br/books?id=c4lAvgAACAAJ.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O que é Ciência de Dados (Data Science)?</span>"
    ]
  },
  {
    "objectID": "qualidade_de_dados.html",
    "href": "qualidade_de_dados.html",
    "title": "3  Qualidade de Dados",
    "section": "",
    "text": "3.1 Pacotes que vamos utilizar\nPara realizar as análises de qualidade de dados, utilizaremos os seguintes pacotes no R, que oferecem ferramentas poderosas para manipulação, visualização e limpeza de dados:\n# Carregando os pacotes necessários\nlibrary(dplyr) # Manipulação de dados\nlibrary(ggplot2) # Visualização de dados\nlibrary(janitor) # Limpeza e resumo dos dados\nlibrary(lubridate) # Manipulação de datas\nlibrary(naniar) # Visualização de dados faltantes\nlibrary(stringr) # Manipulação de textos\nlibrary(tidyr) # Para transformar os dados",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualidade de Dados</span>"
    ]
  },
  {
    "objectID": "qualidade_de_dados.html#qualidade-de-dados-categóricos",
    "href": "qualidade_de_dados.html#qualidade-de-dados-categóricos",
    "title": "3  Qualidade de Dados",
    "section": "3.2 Qualidade de Dados Categóricos",
    "text": "3.2 Qualidade de Dados Categóricos\nA análise de dados categóricos é crucial para entender a distribuição das classes e identificar possíveis desequilíbrios. Um bom balanceamento de classes é essencial para garantir que os modelos de aprendizado de máquina não sejam tendenciosos. Além disso, é importante verificar o percentual de dados ausentes, pois a falta de dados categóricos pode distorcer a análise.\n\n# Carregando o dataset 'iris' e criando dados ausentes para demonstração\ndata(\"iris\")\nset.seed(1234) # Para reprodutibilidade\n\n# Introduzindo NA's aleatoriamente em algumas observações da coluna Species\niris$Species[sample(51:150, 25)] &lt;- NA \n\n# Análise de balanceamento de classes com 'tabyl' do 'janitor'\niris_tabyl &lt;- iris |&gt; \n  janitor::tabyl(Species)\n\n# Exibindo análise\niris_tabyl\n\n    Species  n   percent valid_percent\n     setosa 50 0.3333333         0.400\n versicolor 39 0.2600000         0.312\n  virginica 36 0.2400000         0.288\n       &lt;NA&gt; 25 0.1666667            NA",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualidade de Dados</span>"
    ]
  },
  {
    "objectID": "qualidade_de_dados.html#qualidade-de-dados-numéricos",
    "href": "qualidade_de_dados.html#qualidade-de-dados-numéricos",
    "title": "3  Qualidade de Dados",
    "section": "3.3 Qualidade de Dados Numéricos",
    "text": "3.3 Qualidade de Dados Numéricos\nPara dados numéricos, calcular estatísticas descritivas como média, mediana, desvio padrão, mínimo e máximo é fundamental para entender a distribuição dos dados. Além disso, verificar o percentual de dados ausentes ajuda a identificar problemas que podem afetar a análise. Dados numéricos de baixa qualidade podem levar a modelos preditivos imprecisos.\n\n# Carregando o dataset 'airquality'\ndata(\"airquality\")\n\n# Calculando as estatísticas descritivas e percentual de dados faltantes\nstats &lt;- airquality |&gt;\n  dplyr::select(-c(\"Month\", \"Day\")) |&gt; # Retiradas variáveis que,\n  # apesar de numéricas, sua análise deve ser feita como categoria.\n  dplyr::summarise(\n    dplyr::across(\n      dplyr::where(is.numeric),\n      list(\n        mean = ~mean(., na.rm = TRUE),\n        median = ~median(., na.rm = TRUE),\n        sd = ~sd(., na.rm = TRUE),\n        min = ~min(., na.rm = TRUE),\n        max = ~max(., na.rm = TRUE),\n        na_percentage = ~sum(is.na(.)) / n() * 100\n      ),\n      .names = \"{.col}-{.fn}\"  # Usando hífen como separador\n    )\n  ) |&gt;\n  tidyr::pivot_longer(\n    cols = everything(), \n    names_to = c(\"Variable\", \".value\"), \n    names_sep = \"-\"  # Alinhando o separador com o usado acima\n  )\n\n# Exibindo as estatísticas descritivas\nstats\n\n# A tibble: 4 × 7\n  Variable   mean median    sd   min   max na_percentage\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Ozone     42.1    31.5 33.0    1   168           24.2 \n2 Solar.R  186.    205   90.1    7   334            4.58\n3 Wind       9.96    9.7  3.52   1.7  20.7          0   \n4 Temp      77.9    79    9.47  56    97            0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualidade de Dados</span>"
    ]
  },
  {
    "objectID": "qualidade_de_dados.html#qualidade-de-dados-tipo-texto",
    "href": "qualidade_de_dados.html#qualidade-de-dados-tipo-texto",
    "title": "3  Qualidade de Dados",
    "section": "3.4 Qualidade de Dados Tipo Texto",
    "text": "3.4 Qualidade de Dados Tipo Texto\nA análise de dados textuais envolve verificar o comprimento dos textos, o percentual de campos ausentes e identificar padrões comuns ou preenchimentos preguiçosos. Dados textuais são frequentemente utilizados para análises qualitativas e sentimentais, e a qualidade desses dados pode impactar significativamente os resultados.\n\n# Lendo o dataset\ncustomer_satisfaction_df &lt;- read.csv(\"data/customer_satisfaction.csv\")\n\n# Análise de variáveis texto (coluna Feedback)\ntext_analysis &lt;- customer_satisfaction_df |&gt;\n  summarise(\n    average_length = mean(str_length(Feedback), na.rm = TRUE),\n    max_length = max(str_length(Feedback), na.rm = TRUE),\n    min_length = min(str_length(Feedback), na.rm = TRUE),\n    na_empty_percent = sum(is.na(Feedback) | Feedback == \"\") / n() * 100,\n    unique_char_lines = sum(str_detect(Feedback, \"^([a-zA-Z0-9])\\\\1*$\"), na.rm = TRUE),\n    n = n()\n  )\n\n# Exibindo resultados\ntext_analysis\n\n  average_length max_length min_length na_empty_percent unique_char_lines   n\n1         22.275         45          0               14                58 200",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualidade de Dados</span>"
    ]
  },
  {
    "objectID": "qualidade_de_dados.html#qualidade-de-dados-temporais",
    "href": "qualidade_de_dados.html#qualidade-de-dados-temporais",
    "title": "3  Qualidade de Dados",
    "section": "3.5 Qualidade de Dados Temporais",
    "text": "3.5 Qualidade de Dados Temporais\nPara dados temporais, é essencial identificar as datas mínimas e máximas e o percentual de dados ausentes. Dados temporais são frequentemente utilizados para análises de séries temporais e previsões, e a qualidade desses dados é crucial para a precisão das análises.\n\n# Tendo certeza que 'Data_Entrada' é do tipo data\nstr(customer_satisfaction_df$Data_Entrada)\n\n chr [1:200] \"20222-01-27\" \"202-05-18\" \"2002-02-14\" \" \" \" \" ...\n\n\n\n# Convertendo 'Data_Entrada' para o tipo data com ymd\n# Registros incorretos serão convertidos em NA\ncustomer_satisfaction_df$Data_Entrada &lt;- ymd(customer_satisfaction_df$Data_Entrada)\n\nWarning: 5 failed to parse.\n\nstr(customer_satisfaction_df$Data_Entrada)\n\n Date[1:200], format: NA NA \"2002-02-14\" NA NA NA \"2022-01-20\" \"2022-07-14\" \"2022-10-02\" ...\n\n\n\n# Análise de variáveis data\ndate_analysis &lt;- customer_satisfaction_df %&gt;%\n  summarise(\n    min_date = min(Data_Entrada, na.rm = TRUE),\n    max_date = max(Data_Entrada, na.rm = TRUE),\n    na_percentage = sum(is.na(Data_Entrada)) / n() * 100\n  )\n\n# Exibindo resultados\ndate_analysis\n\n    min_date   max_date na_percentage\n1 2002-02-14 2022-12-31           2.5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualidade de Dados</span>"
    ]
  },
  {
    "objectID": "qualidade_de_dados.html#dados-faltantes-e-remoção-de-outliers",
    "href": "qualidade_de_dados.html#dados-faltantes-e-remoção-de-outliers",
    "title": "3  Qualidade de Dados",
    "section": "3.6 Dados Faltantes e Remoção de Outliers",
    "text": "3.6 Dados Faltantes e Remoção de Outliers\n\n3.6.1 Tratamento dos Dados Faltantes\nDados faltantes são um desafio comum em qualquer análise de dados. Eles podem ser tratados através de remoção ou imputação. A escolha do método depende do contexto e da quantidade de dados ausentes. Remover dados faltantes pode ser apropriado quando a perda de informações é mínima, enquanto a imputação pode ser mais adequada quando se deseja preservar o máximo de dados possível.\n\n3.6.1.1 Remoção\n\n# Removendo linhas com dados faltantes\nairquality |&gt; na.omit()\n\n\n\n3.6.1.2 Imputação\nImputação pode ser feita pela média ou por regressão, dependendo da complexidade desejada. A imputação pela média é simples e mantém a média geral dos dados, enquanto a imputação por regressão pode capturar melhor as relações entre variáveis.\n\n# Imputação dos dados faltantes com a média\nairquality_imputed &lt;- airquality |&gt;\n  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))\n\n\n\n\n3.6.2 Tratamento de Outliers\nOutliers são valores que se desviam significativamente dos outros dados e podem distorcer análises e modelos. A remoção de outliers pode ser feita usando o método do IQR, que é robusto a valores extremos e fornece um método confiável para identificá-los.\n\n# Calculando o IQR para 'Ozone'\nozone_IQR &lt;- IQR(airquality$Ozone, na.rm = TRUE)\nozone_IQR\n\n[1] 45.25\n\n\n\n# Definindo limites para outliers\nlower_bound &lt;- quantile(airquality$Ozone, 0.25, na.rm = TRUE) - 1.5 * ozone_IQR\nupper_bound &lt;- quantile(airquality$Ozone, 0.75, na.rm = TRUE) + 1.5 * ozone_IQR\n\n# Filtrando outliers\nairquality_no_outliers &lt;- airquality |&gt;\n  dplyr::filter(Ozone &gt; lower_bound & Ozone &lt; upper_bound)\n\nGarantir a qualidade dos dados é um processo contínuo e essencial para o sucesso de qualquer projeto de ciência de dados. Ao dividir a análise por tipo de variável e aplicar técnicas específicas para cada uma, podemos melhorar significativamente a integridade e a confiabilidade dos dados, resultando em análises mais precisas e decisões mais informadas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Qualidade de Dados</span>"
    ]
  },
  {
    "objectID": "teste_de_hipoteses.html",
    "href": "teste_de_hipoteses.html",
    "title": "4  Análise de Hipóteses",
    "section": "",
    "text": "Por que Utilizar um Teste de Hipótese?\nQuando as tabelas dinâmicas do Excel já não são suficientes para que os resultados das análises sejam conclusivas é um bom indicativo de que já passou da hora de utilizar testes de hipóteses.\nUm teste de hipótese auxilia a eliminar a incerteza que permanece mesmo após procedimentos de sumarização dos dados, como ocorre nas tabelas dinâmicas dos softwares de planilha eletrônica.\nPor exemplo, uma organização quer saber se os homens possuem salário superior às mulheres em um determinado cargo. A tabela dinâmica vai trazer o resultado, que pode ser expresso através da média dos salários daquele cargo em cada gênero (masculino ou feminino). Porém, o resultado da média pode mostrar apenas uma diferença pequena entre os salários de homens e mulheres, o que acaba deixando no ar a dúvida inicial e nenhuma conclusão pode ser tomada.\nUm teste de hipótese estatística é capaz de dizer (quando bem aplicado) que o salário das mulheres é de fato menor ou maior naquele caso, e que o resultado mostrado na média não é uma simples obra do acaso.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Análise de Hipóteses</span>"
    ]
  },
  {
    "objectID": "teste_de_hipoteses.html#conceitos-importantes-sobre-os-testes-de-hipóteses",
    "href": "teste_de_hipoteses.html#conceitos-importantes-sobre-os-testes-de-hipóteses",
    "title": "4  Análise de Hipóteses",
    "section": "4.1 Conceitos Importantes sobre os Testes de Hipóteses",
    "text": "4.1 Conceitos Importantes sobre os Testes de Hipóteses\nUma hipótese estatística, formalmente, é uma afirmação sobre alguma característica da população. Um teste de hipótese, por sua vez, é um procedimento estatístico para dizer se uma afirmação sobre a população é verdadeira.\nSe a probabilidade de ocorrência de um evento atrelado à hipótese for baixa, então a hipótese é assumida como não verdadeira.\n\nHipótese Nula e Hipótese Alternativa\nTodo teste de hipóteses precisa de uma hipótese nula (\\(H_0\\)) e uma hipótese alternativa (\\(H_1\\)).\nA \\(H_0\\) é uma afirmação que sempre representará uma igualdade. Por exemplo: “a média salarial de homens é igual a média salarial de mulheres”; “colaboradores com maior grade possuem o mesmo tempo de empresa que colaboradores com menor grade”. Veja que a \\(H_0\\) pode ser dividia em duas declarações, que chamaremos de \\(X\\) e \\(Y\\). Nos exemplos citados, as declarações que representam \\(X\\) seriam “média salarial de homens” e “colaboradores com maior grade”; enquanto \\(Y\\) seriam “média salarial de mulheres” e “colaboradores com menor grade”.\nJá quando olhamos para \\(H_1\\), temos três possibilidades de configurações: existe diferença, é maior ou é menor. Olhando para as declarações de \\(X\\) e \\(Y\\), poderíamos ter como hipótese alternativa à \\(H_0\\), citada acima, que “a média salarial de homens é maior que a média salarial de mulheres”. Ou ainda, quando não temos um palpite definido (se maior ou menor), podemos definir \\(H_1\\) como “colaboradores com maior grade possuem tempo de empresa diferentes que colaboradores com menor grade”.\nResumindo, para definirmos as hipóteses nula e alternativa teremos:\n\\(H_0 : X = Y\\)\n\\(H_1 : X \\neq Y ~\\text{ou}~ X &lt; Y ~\\text{ou}~ X &gt; Y\\)\n\n\nQual a Decisão em um Teste de Hipótese?\nEm um teste de hipóteses a decisão será sempre rejeitar a hipótese nula ou não rejeitar a hipótese nula.\nPara isso, os testes geram um p-valor para representar a probabilidade de significância estatística (métrica que vai de 0 a 1). Quanto mais próximo de 0 o p-valor, mais significativa é a diferença testada.\n\nUm baixo p-valor indica forte evidência contra a hipótese nula, o que leva à rejeição de \\(H_0\\)\n\nDe forma geral, recomenda-se rejeitar \\(H_0\\) com p-valor menor que \\(0,05\\) (ou \\(5\\%\\)). Em alguns casos específicos, o nível de significância pode ser de \\(10\\%\\).\n\n\nTestes Paramétricos e não Paramétricos?\nTestes paramétricos assumem que os dados são distribuídos aleatoriamente a partir da população e que seguem uma distribuição normal.\nTestes não paramétricos também assumem que os dados são aleatoriamente distribuídos a partir da população, mas não exigem que sigam uma distribuição normal.\nOs testes não paramétricos, além de não exigirem “normalidade” na distribuição dos dados, também apresentam resultados melhores quando aplicados em amostras pequenas. Por esses motivos, em casos reais normalmente são os preferidos e mais adequados em testes de hipóteses.\nA questão que permanece é como saber se o teste a ser escolhido é paramétrico ou não paramétrico?\nA resposta é simples: se o dado a ser testado segue uma distribuição normal, então recomenda-se utilizar testes paramétricos; caso o dado não possua distribuição normal então utiliza-se testes não paramétricos.\nQuando o dado é normalmente distribuído então alguns testes podem trazer inferências sobre intervalos, pois já se conhece a distribuição. Já os testes não paramétricos não se baseiam na distribuição dos dados.\nTestar a normalidade de uma série de dado é bastante simples pela linguagem R. Uma das possibilidades é utilizar o teste Shapiro-Wilk por meio da função shapiro.test(). O teste possui hipótese nula (\\(H_0\\)) de que o dado é normalmente distribuído, a qual recomenda-se ser rejeitada a um p-valor menor que 0,05. Em outras palavras, se o p-valor for maior que 0,05 então assumimos que o dado é normalmente distribuído e seguimos com testes paramétricos; caso contrário procuramos um teste adequado entre os não paramétricos.\n\n\nTestes Pareados e não Pareados\nEm testes para dados pareados as amostras são dependentes. Aplicam-se, por exemplo, no caso das duas métricas que serão comparadas serem obtidas a partir do mesmo indivíduo, antes e após um tratamento.\n\n\nExemplo de teste pareado\n\nApós um colaborador receber movimentação salarial por mérito, sua produtividade melhora.\n\nPara validar esta hipótese um teste pareado sobre as observações de produtividade deveria ser executado com duas amostras do mesmo colaborador: uma antes e outra depois da movimentação salarial por mérito.\n\nEm testes para dados não pareados os dados são coletados de indivíduos distintos e que pertencem a grupos também distintos. As amostras a serem testadas são independentes.\n\n\nExemplo de teste não pareado\n\nAs avaliações de lideranças na área de TI são inferiores às avaliações na área de Recursos Humanos .\n\nPara validar esta hipótese é necessário um teste não pareado sobre as notas dos líderes em cada área.\n\nVale ainda ressaltar que, em testes para dados pareados, obrigatoriamente o tamanho das amostras deve ser igual (afinal, as amostras devem ser “pares”). Já em testes não pareados os tamanhos das amostras podem ser diferentes.\n\n4.2 Tipos de Testes\nExistem diversos testes de hipóteses, sendo que cada um é mais adequado para uma situação específica. A seguir são apresentados alguns testes (não todos, pois existem diversos) que servem para a maior parte das situações que envolvem testes de hipóteses.\n\n4.2.1 Testes de Proporções\nOs testes de proporções são adequados quando se têm variáveis binárias ou categóricas (ou numéricas divididas em faixas, como renda, idade ou número de funcionários), e se deseja saber se determinada característica é mais ou menos presente em um certo tratamento. Alguns exemplos de hipóteses alternativas a serem verificadas com testes de proporções:\n\nO turnover voluntário é maior em colaboradores do gênero masculino do que feminino;\nHá um maior índice de turnover voluntário em colaboradores cuja frequência de viagem a trabalho é maior.\n\nVeja, turnover é uma variável binária, que indica quando um colaborador é desligado ou não, e o que normalmente se deseja testar com esta variável são casos em que há maior ou menor índice de turnover. Este é um tipo de situação em que é adequado aplicar testes de proporções.\n\nTeste Z\nÉ um teste paramétrico utilizado para comparar diferenças de proporções entre duas amostras independentes. É idêntico ao teste Qui-Quadrado para diferença de proporções (apresentado a seguir), exceto que este permite estimar o desvio-padrão pela distribuição normal. Um cuidado que deve-se tomar com este teste é relacionado a sua aplicação em amostras que não são independentes. A equação implementada na linguagem R para teste de proporções não contempla o teste Z, apenas o Qui-Quadrado.\nUsos e mau usos do Teste Z\n\n\nTeste Qui-Quadrado (\\(X^2\\))\nÉ uma alternativa não paramétrica ao teste Z. O teste Chi-Squared para proporções é um dos testes estatísticos mais utilizados. É mais adequado para amostras pequenas que o teste Z. Um dos seus principais usos incorretos está atrelado também a não independência entre as amostras.\nUsos e mau usos do Person’s Chi-Squared Test\nNa linguagem R há uma função nativa para este teste, prop.test(), que necessita como input (i) a quantidade de ocorrências para cada evento e (ii) o total de casos. Assim, o próprio teste calcula as proporções. Além disso, é possível aplicar o teste também para verificar se há diferença entre mais de duas amostras, e também verificar se há tendência nas proporções entre os grupos, por meio da função prop.trend.test().\n\n\nTeste de Fisher\nO teste exato de Fisher é um teste não paramétrico que tem o objetivo de testar a independência de duas ou mais variáveis categóricas. Ele é uma alternativa ao teste Qui-Quadrado e normalmente é utilizado para a tabela de contingência 2×2, ou quando as frequências esperadas de uma das células da tabela de contingência são menores do que 5.\nA lógica do teste de Fisher é a mesma apresentada no teste Qui-Quadrado: identificar se as variáveis categóricas são independentes (o H0, hipótese nula do teste) ou se exsite alguma relação entre elas (o H1, hipótese alternativa do teste).\nNa linguagem R, o teste de Fisher pode ser facilmente implementado utilizando a função fisher.test, que necessita como input uma tabela de continguência ou a especificação de duas variáveis categorias de uma base de dados, que serão então utilizadas pelo teste para identificar a existência ou não de associações entre elas.\n\n\n\n4.2.2 Testes para Diferenças com uma Amostra (one sample)\nSão testes aplicados para variáveis contínuas em caso de se ter apenas uma amostra de dados e desejar testar se há diferença desta amostra contra parâmetros hipotéticos. Por exemplo:\n\\(H_0\\): os colaboradores recém promovidos recebem em torno de 100% da faixa.\n\\(H_1\\): os colaboradores recém promovidos recebem mais de 100% da faixa.\nNeste caso, não se compara duas ou mais amostras, mas realiza-se o teste com base em um valor que seria esperado por alguma razão, por exemplo, por uma política de recursos humanos da organização.\n\nTeste T One Sample\nO teste T para uma amostra é um teste paramétrico que permite verificar se a média de uma série de dados é diferente de uma média hipotética que se deseja testar. Para implementar o teste em R, utiliza-se t.test(x, mu = 0), em que x representa o vetor com a amostra a ser testada, e mu é o parâmetro para definir a média esperada.\n\n\nWilcoxon Signed Rank\nÉ um teste não paramétrico para uma amostra contra uma mediana hipotética. Foi proposto no mesmo artigo que o teste Wilcoxon Rank Sum, aplicável para duas amostras. Na linguagem R, utiliza-se a função wilcox.test().\n\n\n\n4.2.3 Testes para Diferenças entre Dois Grupos (two sample)\nQuando se têm variáveis contínuas e se deseja verificar uma possível diferença entre duas amostras utiliza-se um teste para verificar se há diferenças entre as distribuições.\n\nTeste T Two Sample\nÉ um teste paramétrico, capaz de verificar se duas populações possuem médias iguais para uma determinada variável. A versão do teste implementada na linguagem R (Welch’s t-test) é mais confiável do que o tradicional Teste t de Student quando as amostras não possuem a mesma variância e/ou o tamanho das amostras é desigual.\n\n\nO gráfico ao lado é um exemplo em que pode se aplicar o seguinte teste:\n\\(H_0\\) : A = B.\n\\(H_1\\) : A &lt; B.\nO gráfico foi construído com uma amostra aleatória que segue a distribuição normal. As médias (\\(\\mu\\)) e desvios-padrão (\\(\\sigma\\)) simuladas estão expressas no gráfico. De fato \\(H_0\\) é rejeitada pelo teste T a um p-valor \\(&lt;\\) 0,05 neste exemplo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara implementar o teste em R, utiliza-se t.test(x,y), em que x e y representam vetores com as duas amostras a serem testadas. Para teste pareado, basta utilizar o parâmetro paired = TRUE dentro da função.\n\n\nWilcoxon Rank Sum\nEste é um teste que serve como alternativa não paramétrica ao teste T para duas amostras, sendo também chamado de Teste U de Mann-Whitney. Muito utilizado para testar diferenças entre duas amostras, pois não é preciso cumprir a premissa de normalidade. Essa característica faz com que este seja um teste mais abrangente que o teste T, servindo para variadas situações do dia a dia.\nÉ um teste baseado apenas na ordem em que as observações das duas amostras aparecem. Um caso de uso interessante ocorre quando a amostra é pequena demais a ponto de não ser possível dizer se a distribuição é normal ou não.\nO teste Rank Sum de Wilcoxon baseia-se na classificação das observações das duas amostras sendo testadas. À cada observação é atribuída uma classificação, sendo que a menor tem classificação 1, a segunda menor classificação 2, e assim por diante. A estatística de teste é calculada com base na soma das classificações de cada uma das amostras. Dessa forma o teste consegue dizer se a soma dos rankings associados a uma amostra é menor, igual ou maior que da outra, apontando se há diferenças nas amostras e também o sentido desta diferença. O resultado é similar ao do teste T, mas por utilizar um sistema de ranking não se presume nada acerca de como o dado é distribuído.\nPara aplicar o teste na linguagem R utiliza-se a função wilcox.test(x, y), em que x e y representam vetores com as duas amostras a serem testadas. Para teste pareado basta utilizar o parâmetro paired = TRUE.\n\n\nO gráfico ao lado é um exemplo em que pode se aplicar o seguinte teste:\n\\(H_0\\) : A = B.\n\\(H_1\\) : A &lt; B.\nO gráfico foi construído com duas amostras de 50 observações cada, com dados que não possuem nitidamente o formato de sino da distribuição normal, apesar de o teste de normalidade não ter fornecido indícios de que o dado não segue uma distribuição normal. Pelo teste Wilcoxon \\(H_0\\) é rejeitada a um p-valor \\(&lt;\\) 0,05.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeja mais sobre o Wilcoxon Rank Sum.\nArtigo comparativo entre teste T e Wilcoxon Rank Sum\n\n\n\n\n4.2.4 Testes para Diferenças entre mais de Dois Grupos\n\nANOVA\nAnálise de Variância (ANOVA, do inglês Analisys of Variance), compreende uma família de testes que permitem verificar diferenças entre séries de dados. A ideia central da ANOVA é testar se há diferença entre as médias (a ANOVA pertence aos testes paramétricos) das amostras analisadas, permitindo trabalhar com 3 ou mais amostras. A Análise de Variância é muito utilizada em ambientes experimentais, para verificar a existência de diferenças entre variados tratamentos aplicados em grupos distintos.\nÉ, portanto, uma alternativa paramétrica ao teste T, para os casos com mais de duas amostras. Porém, por ser paramétrico possui seu uso restrito.\n\n\nKruskal-Wallis\nEste é um teste não paramétrico que serve como alternativa à ANOVA, caso os critérios de normalidade e homoscedasticidade (igual variância entre as amostras) não sejam cumpridos. Kruskal-Wallis é uma extensão do teste Wilcoxon Rank Sum, sendo aplicável para casos com mais de duas amostras a serem comparadas. O resultado do teste indica se há diferença entre pelo menos duas das amostras testadas.\nO teste pode ser executado com amostras extremamente pequenas (a partir de 6 observações, sendo pelo menos duas para cada grupo). Também é um teste aplicável para amostras não balanceadas (tamanhos diferentes).\n\n\nKruskal-Wallis Dunn Test\nQuando a hipótese nula do teste Kruskal-Wallis é rejeitada, então sabe-se que pelo menos duas amostras são distintas (das 3 ou mais que foram testadas). Porém, na maioria dos casos ainda se deseja saber mais. A conclusão que de fato vai agregar valor é quais as combinações de grupos que diferem, e quais são os sinais destas diferenças. É para esta situação que utiliza-se o Dunn Test.\n\n\n\n\n4.3 Qual teste utilizar em cada caso?\nDado que existem diversos testes de hipótese e que cada um é mais adequado para um determinado tipo de situação, a seguir é apresentado um fluxograma que auxilia na escolha do teste.\n\nPor fim, a tabela a seguir sintetiza o que foi abordado neste documento acerca de testes de hipóteses. Existem diversos outros testes, porém, os que são apresentados na sequência viabilizam a verificação estatística da maior parte das hipóteses de negócio levantas em projetos de análise de dados.\n\n\n\nTeste\nTesta diferenças entre:\nParamétrico ou não?\nQtde de amostras que compara\nFunção no R / pacote\nObservações\n\n\n\n\nZ\nProporções\nParamétrico\n2\nNecessário criar\n\n\n\nChi-Squared\nProporções\nNão paramétrico\n2\nprop.test() / Default\n\n\n\nT one sample\nMédias\nParamétrico\n2\nt.test() / Default\nPareado\n\n\nT two sample\nMédias\nParamétrico\n2\nt.test() / Default\nNão pareado\n\n\nWilcoxon one sample\nDistribuições\nNão paramétrico\n\nwilcox.test() / Default\nPareado, análogo ao teste T one sample.\n\n\nWilcoxon rank sum test (ou Mann Whitney U Test)\nDistribuições\nNão paramétrico\n2\nwilcox.test() / Default\nNão pareado, análogo ao teste T two sample.\n\n\nKruskal-Wallis\nDistribuições\nNão paramétrico\n3 ou mais\nkruskal.test / Default\nNão pareado. Extensão do Wilcoxon rank sum. Se pelo menos dois grupos apresentarem diferença, então o p-valor é significativo.\n\n\nDunn’s Kruskal-Wallis\nDistribuições\nNão paramétrico\nTesta todas as combinações em pares\ndunnTest() / FSA\nQuando o Kruskal-Wallis é significativo, o Dunn’s test verifica em quais combinações há diferença.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Análise de Hipóteses</span>"
    ]
  },
  {
    "objectID": "teste_de_hipoteses.html#tipos-de-testes",
    "href": "teste_de_hipoteses.html#tipos-de-testes",
    "title": "4  Análise de Hipóteses",
    "section": "4.2 Tipos de Testes",
    "text": "4.2 Tipos de Testes\nExistem diversos testes de hipóteses, sendo que cada um é mais adequado para uma situação específica. A seguir são apresentados alguns testes (não todos, pois existem diversos) que servem para a maior parte das situações que envolvem testes de hipóteses.\n\n4.2.1 Testes de Proporções\nOs testes de proporções são adequados quando se têm variáveis binárias ou categóricas (ou numéricas divididas em faixas, como renda, idade ou número de funcionários), e se deseja saber se determinada característica é mais ou menos presente em um certo tratamento. Alguns exemplos de hipóteses alternativas a serem verificadas com testes de proporções:\n\nO turnover voluntário é maior em colaboradores do gênero masculino do que feminino;\nHá um maior índice de turnover voluntário em colaboradores cuja frequência de viagem a trabalho é maior.\n\nVeja, turnover é uma variável binária, que indica quando um colaborador é desligado ou não, e o que normalmente se deseja testar com esta variável são casos em que há maior ou menor índice de turnover. Este é um tipo de situação em que é adequado aplicar testes de proporções.\n\nTeste Z\nÉ um teste paramétrico utilizado para comparar diferenças de proporções entre duas amostras independentes. É idêntico ao teste Qui-Quadrado para diferença de proporções (apresentado a seguir), exceto que este permite estimar o desvio-padrão pela distribuição normal. Um cuidado que deve-se tomar com este teste é relacionado a sua aplicação em amostras que não são independentes. A equação implementada na linguagem R para teste de proporções não contempla o teste Z, apenas o Qui-Quadrado.\nUsos e mau usos do Teste Z\n\n\nTeste Qui-Quadrado (\\(X^2\\))\nÉ uma alternativa não paramétrica ao teste Z. O teste Chi-Squared para proporções é um dos testes estatísticos mais utilizados. É mais adequado para amostras pequenas que o teste Z. Um dos seus principais usos incorretos está atrelado também a não independência entre as amostras.\nUsos e mau usos do Person’s Chi-Squared Test\nNa linguagem R há uma função nativa para este teste, prop.test(), que necessita como input (i) a quantidade de ocorrências para cada evento e (ii) o total de casos. Assim, o próprio teste calcula as proporções. Além disso, é possível aplicar o teste também para verificar se há diferença entre mais de duas amostras, e também verificar se há tendência nas proporções entre os grupos, por meio da função prop.trend.test().\n\n\nTeste de Fisher\nO teste exato de Fisher é um teste não paramétrico que tem o objetivo de testar a independência de duas ou mais variáveis categóricas. Ele é uma alternativa ao teste Qui-Quadrado e normalmente é utilizado para a tabela de contingência 2×2, ou quando as frequências esperadas de uma das células da tabela de contingência são menores do que 5.\nA lógica do teste de Fisher é a mesma apresentada no teste Qui-Quadrado: identificar se as variáveis categóricas são independentes (o H0, hipótese nula do teste) ou se exsite alguma relação entre elas (o H1, hipótese alternativa do teste).\nNa linguagem R, o teste de Fisher pode ser facilmente implementado utilizando a função fisher.test, que necessita como input uma tabela de continguência ou a especificação de duas variáveis categorias de uma base de dados, que serão então utilizadas pelo teste para identificar a existência ou não de associações entre elas.\n\n\n\n4.2.2 Testes para Diferenças com uma Amostra (one sample)\nSão testes aplicados para variáveis contínuas em caso de se ter apenas uma amostra de dados e desejar testar se há diferença desta amostra contra parâmetros hipotéticos. Por exemplo:\n\\(H_0\\): os colaboradores recém promovidos recebem em torno de 100% da faixa.\n\\(H_1\\): os colaboradores recém promovidos recebem mais de 100% da faixa.\nNeste caso, não se compara duas ou mais amostras, mas realiza-se o teste com base em um valor que seria esperado por alguma razão, por exemplo, por uma política de recursos humanos da organização.\n\nTeste T One Sample\nO teste T para uma amostra é um teste paramétrico que permite verificar se a média de uma série de dados é diferente de uma média hipotética que se deseja testar. Para implementar o teste em R, utiliza-se t.test(x, mu = 0), em que x representa o vetor com a amostra a ser testada, e mu é o parâmetro para definir a média esperada.\n\n\nWilcoxon Signed Rank\nÉ um teste não paramétrico para uma amostra contra uma mediana hipotética. Foi proposto no mesmo artigo que o teste Wilcoxon Rank Sum, aplicável para duas amostras. Na linguagem R, utiliza-se a função wilcox.test().\n\n\n\n4.2.3 Testes para Diferenças entre Dois Grupos (two sample)\nQuando se têm variáveis contínuas e se deseja verificar uma possível diferença entre duas amostras utiliza-se um teste para verificar se há diferenças entre as distribuições.\n\nTeste T Two Sample\nÉ um teste paramétrico, capaz de verificar se duas populações possuem médias iguais para uma determinada variável. A versão do teste implementada na linguagem R (Welch’s t-test) é mais confiável do que o tradicional Teste t de Student quando as amostras não possuem a mesma variância e/ou o tamanho das amostras é desigual.\n\n\nO gráfico ao lado é um exemplo em que pode se aplicar o seguinte teste:\n\\(H_0\\) : A = B.\n\\(H_1\\) : A &lt; B.\nO gráfico foi construído com uma amostra aleatória que segue a distribuição normal. As médias (\\(\\mu\\)) e desvios-padrão (\\(\\sigma\\)) simuladas estão expressas no gráfico. De fato \\(H_0\\) é rejeitada pelo teste T a um p-valor \\(&lt;\\) 0,05 neste exemplo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara implementar o teste em R, utiliza-se t.test(x,y), em que x e y representam vetores com as duas amostras a serem testadas. Para teste pareado, basta utilizar o parâmetro paired = TRUE dentro da função.\n\n\nWilcoxon Rank Sum\nEste é um teste que serve como alternativa não paramétrica ao teste T para duas amostras, sendo também chamado de Teste U de Mann-Whitney. Muito utilizado para testar diferenças entre duas amostras, pois não é preciso cumprir a premissa de normalidade. Essa característica faz com que este seja um teste mais abrangente que o teste T, servindo para variadas situações do dia a dia.\nÉ um teste baseado apenas na ordem em que as observações das duas amostras aparecem. Um caso de uso interessante ocorre quando a amostra é pequena demais a ponto de não ser possível dizer se a distribuição é normal ou não.\nO teste Rank Sum de Wilcoxon baseia-se na classificação das observações das duas amostras sendo testadas. À cada observação é atribuída uma classificação, sendo que a menor tem classificação 1, a segunda menor classificação 2, e assim por diante. A estatística de teste é calculada com base na soma das classificações de cada uma das amostras. Dessa forma o teste consegue dizer se a soma dos rankings associados a uma amostra é menor, igual ou maior que da outra, apontando se há diferenças nas amostras e também o sentido desta diferença. O resultado é similar ao do teste T, mas por utilizar um sistema de ranking não se presume nada acerca de como o dado é distribuído.\nPara aplicar o teste na linguagem R utiliza-se a função wilcox.test(x, y), em que x e y representam vetores com as duas amostras a serem testadas. Para teste pareado basta utilizar o parâmetro paired = TRUE.\n\n\nO gráfico ao lado é um exemplo em que pode se aplicar o seguinte teste:\n\\(H_0\\) : A = B.\n\\(H_1\\) : A &lt; B.\nO gráfico foi construído com duas amostras de 50 observações cada, com dados que não possuem nitidamente o formato de sino da distribuição normal, apesar de o teste de normalidade não ter fornecido indícios de que o dado não segue uma distribuição normal. Pelo teste Wilcoxon \\(H_0\\) é rejeitada a um p-valor \\(&lt;\\) 0,05.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeja mais sobre o Wilcoxon Rank Sum.\nArtigo comparativo entre teste T e Wilcoxon Rank Sum\n\n\n\n\n4.2.4 Testes para Diferenças entre mais de Dois Grupos\n\nANOVA\nAnálise de Variância (ANOVA, do inglês Analisys of Variance), compreende uma família de testes que permitem verificar diferenças entre séries de dados. A ideia central da ANOVA é testar se há diferença entre as médias (a ANOVA pertence aos testes paramétricos) das amostras analisadas, permitindo trabalhar com 3 ou mais amostras. A Análise de Variância é muito utilizada em ambientes experimentais, para verificar a existência de diferenças entre variados tratamentos aplicados em grupos distintos.\nÉ, portanto, uma alternativa paramétrica ao teste T, para os casos com mais de duas amostras. Porém, por ser paramétrico possui seu uso restrito.\n\n\nKruskal-Wallis\nEste é um teste não paramétrico que serve como alternativa à ANOVA, caso os critérios de normalidade e homoscedasticidade (igual variância entre as amostras) não sejam cumpridos. Kruskal-Wallis é uma extensão do teste Wilcoxon Rank Sum, sendo aplicável para casos com mais de duas amostras a serem comparadas. O resultado do teste indica se há diferença entre pelo menos duas das amostras testadas.\nO teste pode ser executado com amostras extremamente pequenas (a partir de 6 observações, sendo pelo menos duas para cada grupo). Também é um teste aplicável para amostras não balanceadas (tamanhos diferentes).\n\n\nKruskal-Wallis Dunn Test\nQuando a hipótese nula do teste Kruskal-Wallis é rejeitada, então sabe-se que pelo menos duas amostras são distintas (das 3 ou mais que foram testadas). Porém, na maioria dos casos ainda se deseja saber mais. A conclusão que de fato vai agregar valor é quais as combinações de grupos que diferem, e quais são os sinais destas diferenças. É para esta situação que utiliza-se o Dunn Test.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Análise de Hipóteses</span>"
    ]
  },
  {
    "objectID": "teste_de_hipoteses.html#qual-teste-utilizar-em-cada-caso",
    "href": "teste_de_hipoteses.html#qual-teste-utilizar-em-cada-caso",
    "title": "4  Análise de Hipóteses",
    "section": "4.3 Qual teste utilizar em cada caso?",
    "text": "4.3 Qual teste utilizar em cada caso?\nDado que existem diversos testes de hipótese e que cada um é mais adequado para um determinado tipo de situação, a seguir é apresentado um fluxograma que auxilia na escolha do teste.\n\nPor fim, a tabela a seguir sintetiza o que foi abordado neste documento acerca de testes de hipóteses. Existem diversos outros testes, porém, os que são apresentados na sequência viabilizam a verificação estatística da maior parte das hipóteses de negócio levantas em projetos de análise de dados.\n\n\n\nTeste\nTesta diferenças entre:\nParamétrico ou não?\nQtde de amostras que compara\nFunção no R / pacote\nObservações\n\n\n\n\nZ\nProporções\nParamétrico\n2\nNecessário criar\n\n\n\nChi-Squared\nProporções\nNão paramétrico\n2\nprop.test() / Default\n\n\n\nT one sample\nMédias\nParamétrico\n2\nt.test() / Default\nPareado\n\n\nT two sample\nMédias\nParamétrico\n2\nt.test() / Default\nNão pareado\n\n\nWilcoxon one sample\nDistribuições\nNão paramétrico\n\nwilcox.test() / Default\nPareado, análogo ao teste T one sample.\n\n\nWilcoxon rank sum test (ou Mann Whitney U Test)\nDistribuições\nNão paramétrico\n2\nwilcox.test() / Default\nNão pareado, análogo ao teste T two sample.\n\n\nKruskal-Wallis\nDistribuições\nNão paramétrico\n3 ou mais\nkruskal.test / Default\nNão pareado. Extensão do Wilcoxon rank sum. Se pelo menos dois grupos apresentarem diferença, então o p-valor é significativo.\n\n\nDunn’s Kruskal-Wallis\nDistribuições\nNão paramétrico\nTesta todas as combinações em pares\ndunnTest() / FSA\nQuando o Kruskal-Wallis é significativo, o Dunn’s test verifica em quais combinações há diferença.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Análise de Hipóteses</span>"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "5  Machine Learning para negócios",
    "section": "",
    "text": "5.1 O que é aprendizado de máquina?\nO aprendizado de máquina ocorre quando um algoritmo ou um programa de computador consegue melhorar sua performance nas tarefas que desempenha com base na experiência, utilizando inteligência artificial. Essa experiência ocorre com a alimentação de dados e informações colhidas a partir de interações com o mundo real.\nDessa forma, podemos dividir o aprendizado de máquina em supervisionado, que busca responder um target, ou seja, há uma variável explícita a ser respondida; e não supervisionado, em que busca-se identificar grupos ou padrões a partir dos dados, sem um objetivo específico a ser alcançado.\nNa sequência do capítulo veja os principais conceitos, métodos e exemplos introdutórios de aplicações de aprendizado de máquina na linguagem R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#o-que-é-aprendizado-de-máquina",
    "href": "machine_learning.html#o-que-é-aprendizado-de-máquina",
    "title": "5  Machine Learning para negócios",
    "section": "",
    "text": "5.1.1 Como machine learning gera valor nos negócios?\nAprendizado de máquina é composto por um conjunto de métodos. Sendo assim, para identificar qual tipo de abordagem de machine learning é mais adequada a cada situação é preciso entender muito bem a pergunta de negócio que busca-se resolver.\nNeste contexto vale ressaltar que o mais importante não é o método em si.\nA questão é: não existem problemas de aprendizado de máquina para serem resolvidos, mas sim problemas práticos de negócio. Por isso, o foco deve ser mantido em soluções que atendam às dores sentidas pelas empresas e que de fato vão resolver problemas reais.\nAssim, o aprendizado de máquina irá gerar muito valor para as empresas.\n\n\n5.1.2 Aprendizado de máquina supervisionado\nEm termos de machine learning aplicado a empresas, quando se tem uma pergunta de negócio do tipo “Qual ação de marketing mais impacta nas vendas?” estamos falando de aprendizado de máquina supervisionado, pois estamos tentando identificar as causas que impactam na variável “vendas”.\nNos modelos de aprendizagem de máquina supervisionada conseguimos dar pesos ou calibrar o nível de assertividade e de precisão de um modelo. A maior parte dos modelos de machine learning são supervisionados.\nSão dividos em:\n\nRegressão: métodos de regressão buscam encontrar como uma variável evolui em relação a outras. Estão entre os métodos mais comuns e mais ensinados nas aulas de estatística nas universidades.\nClassificação: são métodos que buscam explicar uma variável categórica, com duas categorias (variável binária) ou mais.\n\nPodem existir ainda modelos de aprendizado de máquina mistos, que podem utilizar partes de regressão para fazer classificação, ou o oposto.\n\n\n5.1.3 Aprendizado de máquina não supervisionado\nQuando a pergunta de negócio é algo do tipo “Quais os perfis de clientes que compram em uma loja?” estamos falando de aprendizado de máquina não supervisionado. Neste caso, portanto, não temos uma variável específica a ser respondida, pois estamos apenas buscando encontrar os indivíduos, itens ou elementos semelhantes.\nModelos não supervisionados são mais raros na prática, mas são muito úteis para guiar o raciocínio do cientista de dados no processo de exploração dos dados para análises futuras.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#clustering",
    "href": "machine_learning.html#clustering",
    "title": "5  Machine Learning para negócios",
    "section": "6.1 Clustering",
    "text": "6.1 Clustering\nModelos de Clustering referem-se a técnicas de agrupamento, dentro de aprendizado não supervisionado.\n\nÉ uma abordagem para encontrar perfis similares, sem buscar a explicação para uma variável especificamente.\n\nExemplos de usos:\n\nIdentificar espécies de plantas;\nIdentificar áreas de maior criminalidade;\nSegmentação de clientes para encontrar características comuns em grupos de clientes. Pode auxiliar, por exemplo, na análise de churn (perfis de clientes que mais cancelam);\nDetecção de fraudes (indivíduos outliers, cujas combinações de atributos não se enquadram nos clusters).\n\nPara exemplificar os métodos de clustering será utilizado o modelo K-means.\nComo funciona o K-means?\n\nFaz o particionamento de n observações em k clusters.\nAdiciona cada observação ao cluster cujo centróide esteja mais próximo (tecnicamente é a menor distância euclidiana).\nDefine os centróides por iterações que minimizam as distâncias.\nÉ preciso escolher o número de clusters.\n\n\n6.1.1 Aplicando K-Means a dados gerados aleatoriamente\nInicialmente criamos uma matriz com duas variáveis aleatórias. Ambas variáveis de dados aleatórios possuem desvio-padrão de 0,3. Porém uma delas possui média 0 e a outra média 1.\n\n# Definindo o seed\nset.seed(123)\n\n# Criando 100 linhas de dados aleatórios, com uma variável X e Y\ndf_clustering &lt;- dplyr::tibble(\n    x = rnorm(500, mean = 0, sd = 0.3),\n    y = rnorm(500, mean = 0, sd = 0.3)\n  ) |&gt;\n  dplyr::bind_rows(\n    dplyr::tibble(\n      x = rnorm(500, mean = 1, sd = 0.3),\n      y = rnorm(500, mean = 1, sd = 0.3)\n    )\n  )\n\n# Plotando o scatter plot\nhighcharter::hchart(\n  df_clustering,\n  name= \"Dados aleatórios\", \n  type = \"scatter\",\n  hcaes(x = \"x\", y = \"y\"),\n  showInLegend = FALSE\n) %&gt;% \n  highcharter::hc_title(\n    text = \"Dados aleatórios com variâncias diferentes\"\n  ) %&gt;% \n  highcharter::hc_size(height = 400)\n\n\n\n\n\nEntão, aplicamos o K-Means com dois centróides, com o fim de dividir o conjunto de dados em dois clusters.\n\n# Aplicando o K-Means com dois centróides\ncl &lt;- kmeans(\n  x = df_clustering,\n  centers = 2\n)\n\n# Unindo as colunas do dataset de variáveis aleatórias com os clusters\ndf_clustering &lt;- cbind(df_clustering, cl = cl$cluster) %&gt;% \n  mutate(cl = as.factor(cl))\n\nVeja como ficou a quantidade de linhas de dados distribuídas nos dois cluster solicitados:\n\ndf_clustering %&gt;% dplyr::group_by(cl) %&gt;% dplyr::tally()\n\n# A tibble: 2 × 2\n  cl        n\n  &lt;fct&gt; &lt;int&gt;\n1 1       494\n2 2       506\n\n\nNa sequência, o gráfico do tipo scatter plot identificando visualmente cada um dos dois clusters criados.\n\n# Criando scatter plot com identificação de cores para cada cluster\nhighcharter::hchart(\n  df_clustering,\n  name = \"Dados aleatórios\", \n  type = \"scatter\",\n  hcaes(x = \"x\", y = \"y\", group = \"cl\"),\n  showInLegend = FALSE\n) %&gt;% \n  highcharter::hc_title(\n    text = \"Clustering (K-means) de dados aleatórios com variâncias diferentes\"\n  ) %&gt;% \n  highcharter::hc_size(height = 400)\n\n\n\n\n\n\nExercício\n\nAltere a quantidade de observações de 100 para 1000 e veja como o K-Means se comporta.\nAltere o desvio-padrão de 0,3 para 3 (mantendo 1000 observações) e veja como o K-Means se comporta.\n\n\n\n\n6.1.2 Aplicando K-Means para o Iris dataset\nIris dataset é um conjunto de dados de 150 medidas realizadas em 3 espécies diferentes de um tipo de flor (Iris). É um conjunto de dados muito usado para testes em algoritmos de reconhecimento de padrões. Um dos pontos que torna este conjunto de dados interessante é o fato de que uma espécie é linearmente separável das outras duas, mas as duas restantes não podem ser separadas de forma linear. Veja mais.\n\n6.1.2.1 Conhecendo o conjunto de dados\nVeja o resumo dos dados:\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nA seguir, seguem scatter plots comparando as 4 variáveis em questão de duas em duas para entender melhor os dados. Primeiro observe os gráficos sem classificá-los em grupos e na sequência classificados por grupos.\nhchart(\n  iris,\n  name = \"Iris\",\n  type = \"scatter\",\n  hcaes(x = Petal.Length, y = Petal.Width),\n  showInLegend = FALSE\n) %&gt;% \n  highcharter::hc_size(height = 300)\nhchart(\n  iris,\n  name = \"Iris\",\n  type = \"scatter\",\n  hcaes(x = Sepal.Length, y = Sepal.Width),\n  showInLegend = FALSE\n) %&gt;%\n  highcharter::hc_size(height = 300) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOlhando apenas os gráficos acima, quantos grupos você chutaria que existem nestes dados?\nAgora veja os mesmos dados, mas com a cor indicando os grupos de flores:\n# Relacionando Sepal Length e Sepal Width\nhchart(\n  iris,\n  type = \"scatter\",\n  hcaes(\n    x = Petal.Length,\n    y = Petal.Width,\n    group = Species\n  ),\n  showInLegend = TRUE\n) %&gt;%\n  highcharter::hc_size(height = 300)\n# Relacionando Sepal Length e Sepal Width\nhchart(\n  iris,\n  type = \"scatter\",\n  hcaes(\n    x = Sepal.Length,\n    y = Sepal.Width,\n    group = Species\n  ),\n  showInLegend = TRUE\n) %&gt;%\n  highcharter::hc_size(height = 300) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.2.2 Pré-processamento: normalizando os dados\n\n# Separando a colunas de espécies das colunas numéricas\niris.new &lt;- iris |&gt;\n  dplyr::select(\n    Sepal.Length,\n    Sepal.Width,\n    Petal.Length,\n    Petal.Width\n  )\niris.class &lt;- iris[,\"Species\"]\nhead(iris.new)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\n\nSe for o caso de usar uma função para normalizar os dados, o código está comentado abaixo.\n\n# # Criando função de normalização, se for o caso.\n# normalize &lt;- function(x){\n#   return ((x-min(x))/(max(x)-min(x)))\n# }\n# \n# # Aplicando a função de normalização a todas as colunas, se for o caso.\n# iris.new &lt;- iris.new %&gt;% mutate_all(normalize)\n# \n# head(iris.new)\n\n\n\n6.1.2.3 Aplicando o K-means\nDefinindo o número de centróides o dado, já na função do kmeans:\n\n# Aplicar K-Means com 3 centróides\nresult &lt;- kmeans(\n  x = iris.new,\n  centers = 3\n) \n\nVeja os tamanhos de cada cluster final, lembrando que o tamanho real é de 50 observações para espécie.\n\nresult$size \n\n[1] 62 50 38\n\n\nVejamos como ficaram os valores dos centróides para cada uma das 4 variáveis.\n\nresult$centers # retorna o valor do ponto central de cada cluster (para 3 centróides, k=3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.901613    2.748387     4.393548    1.433871\n2     5.006000    3.428000     1.462000    0.246000\n3     6.850000    3.073684     5.742105    2.071053\n\n\nPor fim, conseguimos também obter clusters definidos para cada linha de dado pelo algoritmo.\n\nresult$cluster \n\n  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 3 3 3 1 3 3 3 3\n[112] 3 3 1 1 3 3 3 3 1 3 1 3 1 3 3 1 1 3 3 3 3 3 1 3 3 3 3 1 3 3 3 1 3 3 3 1 3\n[149] 3 1\n\n\n\n\n\n6.1.3 Verificação dos resultados\nO código abaixo apenas cria os gráficos que seguem, comparando as classificações da clusterização pelo K-means com os rótulos reais da coluna “Species”.\n\n# Concatenando coluna  de cluster criado \niris.new.cl &lt;- cbind(\n  iris.new,   \n  \"cl.kmeans\" = result$cluster,\n  \"Species\" = iris.class\n) %&gt;%\n  # Alterando nomes das Species para ficar com as mesmas cores nos gráficos de Cluster x Real\n  dplyr::mutate(\n    \"Species\" =\n      dplyr::case_when(\n        Species == \"versicolor\" ~ \"1. versicolor\",\n        Species == \"virginica\" ~ \"2. virginica\",\n        Species == \"setosa\" ~ \"3. setosa\"\n      )\n  )\n\n# Relacionando Sepal Length e Sepal Width - CLUSTER\nhchart(\n  iris.new.cl,\n  type = \"scatter\",\n  hcaes(\n    x = \"Sepal.Length\",\n    y = \"Sepal.Width\",\n    group = \"cl.kmeans\"\n  ),\n  showInLegend = TRUE\n) %&gt;%\n  highcharter::hc_size(height = 300) %&gt;%\n  hc_title(text = \"Clusters\")\n# Relacionando Sepal Length e Sepal Width - REAL\nhchart(\n  iris.new.cl,\n  type = \"scatter\",\n  hcaes(\n    x = \"Sepal.Length\",\n    y = \"Sepal.Width\",\n    group = \"Species\"\n  ),\n  showInLegend = TRUE\n) %&gt;%\n  highcharter::hc_size(height = 300) %&gt;%\n  hc_title(text = \"Real\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Relacionando Petal Length e Petal Width - CLUSTER\nhchart(\n  iris.new.cl,\n  type = \"scatter\",\n  hcaes(\n    x = \"Petal.Length\",\n    y = \"Petal.Width\",\n    group = \"cl.kmeans\"\n  ),\n  showInLegend = TRUE\n) %&gt;%\n  highcharter::hc_size(height = 300) %&gt;%\n  hc_title(text = \"Clusters\")\n# Relacionando Petal Length e Petal Width - REAL\nhchart(\n  iris.new.cl,\n  type = \"scatter\",\n  hcaes(\n    x = \"Petal.Length\",\n    y = \"Petal.Width\",\n    group = \"Species\"\n  ),\n  showInLegend = TRUE\n) %&gt;%\n  highcharter::hc_size(height = 300) %&gt;%\n  hc_title(text = \"Real\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nO objetivo do aprendizado não supervisionado é encontrar grupos similares quando não se tem um rótulo. Contudo, como este é um exemplo didático e temos os “rótulos”, ou seja, as espécies de cada flor (linha de dado), então podemos inferir as espécies a que cada cluster se refere se olharmos para a tabela de contigência a seguir:\n\ntable(result$cluster,iris.class)\n\n   iris.class\n    setosa versicolor virginica\n  1      0         48        14\n  2     50          0         0\n  3      0          2        36\n\n\nOs resultados foram:\n\nCluster 1 corresponde à espécie “Versicolor”.\nCluster 2 corresponde à espécie “Virgínica”.\nCluster 3 corresponde à espécie “Setosa”.\n\n\n\n6.1.4 Fontes\n\nhttp://rpubs.com/Nitika/kmeans_Iris\nhttps://rpubs.com/AnanyaDu/361293",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#principal-component-analysis-pca",
    "href": "machine_learning.html#principal-component-analysis-pca",
    "title": "5  Machine Learning para negócios",
    "section": "6.2 Principal Component Analysis (PCA)",
    "text": "6.2 Principal Component Analysis (PCA)\nPodemos comparar duas variáveis (colunas) de cada vez se usarmos diagramas de dispersão (scatter plots). Porém, veja a quantidade necessária de scatter plots dependendo da quantidade de variáveis a serem comparadas:\n\n2 variáveis (A,B) = 1 scatter = (AxB)\n3 variáveis (A,B,C) = 3 scatters = (AxB), (AxC), (BxC)\n4 variáveis (A,B,C,D) = 6 scatters = (AxB), (AxC), (AxD), (BxC), (BxD), (CxD)\n\nAlém da enorme quantidade de scatter plots necessários, fica extremamente inviável conseguir analisar todos esses gráficos e tirar valor deles com análises visuais.\nO PCA, neste sentido, busca auxiliar neste problema, pois é capaz de resumir as variáveis de um conjunto de dados em “componentes principais” que contém a maior parte da variância das variáveis originais.\nO n dos componentes principais é igual ao número de variáveis originais. Contudo, normalmente em poucos componentes praticamente toda correlação das variáveis originais é “absorvida” e 2 ou 3 componentes contemplam praticamente toda a variância do conjunto de dados.\nOs componentes principais (PC’s) são classificados por ordem de explicação da variância. Sendo assim, o PC1 explica mais a variabilidade dos dados do que o PC2, e assim por diante.\nDessa forma, um procedimento que costuma-se fazer para visualizar melhor o impacto das variáveis com o auxílio do PCA é utilizar gráficos de dispersão entre 2 PC’s de cada vez. Além disso, também é possível mostrar a direção para a qual cada variável original aponta no espaço 2D de cada scatter plot. Este tipo de gráfico é chamado de biplot.\nExemplos de usos:\n\nAnálise exploratória, como identificar visualmente no biplots como as variáveis se relacionam dentro dos componentes.\nRedução de dimensionalidade: reduz o tempo de processamento e pode melhorar os resultados de modelos preditivos ao usar PCA nos dados de treino e teste.\nCompressão de imagens, entre outros.\n\nComo funciona o PCA?\nPartimos de um dataset com as variáveis originais e chegamos a um dataset reduzido a “compontes principais”.\n\nA grosso modo, procura encontrar “linhas retas” que melhor expressam as direções dos dados em multi-dimensões.\nNão é recomendado para variáveis categóricas (apenas numéricas).\nSe concentra nas direções que definem a maior parte da variância dos dados.\n\nAutovetor (eigenvector): é a direção (vertical ou 45 graus).\nAutovalor (eigenvalue): é um número que expressa o quanto há de variância naquela direção.\n\n\nO autovetor com maior autovalor é o primeiro componente principal.\nVeja mais\n\n6.2.1 Aplicando PCA no conjunto de dados mtcars\nO conjunto de dados “Motor Trend Car Road Tests” foi extraído de uma revista automobilística de 1974 e contempla 10 variáveis que representam características ou métricas de desempenho dos veículos. Os dados são nativos do R, basta digitar mtcars no console. Pra informações, como dicionário de dados, basta digitar ? mtcars, e accessar a documentação de ajuda.\nVeja o head dos dados:\n\ndata &lt;- mtcars %&gt;% dplyr::select(-c(\"vs\", \"am\"))\n\ndata %&gt;% head(5)\n\n                   mpg cyl disp  hp drat    wt  qsec gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02    3    2\n\n\nAplicando a função prcomp, nativa do R Stats, a qual executa uma análise dos componentes principais.\n\nmtcars.pca &lt;- prcomp(data, center = TRUE, scale. = TRUE)\n\nCada análise de PCA resulta em um número de componentes principais igual ao número de variáveis. Porém, não pode-se dizer que um PCA é análogo a uma determinada variável.\nA seguir, é apresentado resumo da importância de cada um dos componentes principais identificados.\n\nsummary(mtcars.pca)$importance \n\n                            PC1      PC2       PC3       PC4       PC5\nStandard deviation     2.378222 1.442948 0.7100809 0.5148082 0.4279704\nProportion of Variance 0.628440 0.231340 0.0560200 0.0294500 0.0203500\nCumulative Proportion  0.628440 0.859780 0.9158100 0.9452500 0.9656000\n                             PC6       PC7       PC8       PC9\nStandard deviation     0.3518426 0.3241326 0.2418962 0.1489644\nProportion of Variance 0.0137500 0.0116700 0.0065000 0.0024700\nCumulative Proportion  0.9793600 0.9910300 0.9975300 1.0000000\n\n\nVeja que os 4 primeiros componentes principais já são responsáveis por 95% da porporção da variância dos dados.\nSegue o head dos 4 primeiros componentes principais, em que cada linha é uma linha de dado (um carro) e cada coluna é substituída por um componente principal (que não mais representa nenhuma coluna especificamente, mas contém variância “misturada” das variáveis originais):\n\nmtcars.pca$x[,1:4] %&gt;% head(10) \n\n                          PC1        PC2        PC3         PC4\nMazda RX4         -0.66422351  1.1734476 -0.2043172 -0.12601751\nMazda RX4 Wag     -0.63719807  0.9769448  0.1107778 -0.08567709\nDatsun 710        -2.29973601 -0.3265893 -0.2101495 -0.10862524\nHornet 4 Drive    -0.21529670 -1.9768101 -0.3294682 -0.30806225\nHornet Sportabout  1.58697405 -0.8287285 -1.0329925  0.14738418\nValiant            0.04960512 -2.4466838  0.1117777 -0.87154914\nDuster 360         2.71439677  0.3610529 -0.6520604  0.09633337\nMerc 240D         -2.04370658 -0.8006412  0.8489880 -0.27451338\nMerc 230          -2.29506729 -1.3056004  1.9684845  0.05055875\nMerc 280          -0.38252133  0.5811211  0.8863227  0.07026946\n\n\n\n\n6.2.2 Visualizando graficamente os resultados do PCA\nÉ possível também criar um biplot para visualizar como cada variável impacta nas relações entre os componentes principais. Para isso vamos usar novamente o pacote de visualização highcharter.\nAgora, veja o gráfico biplot que compara o PC1 e PC2 e mostra como cada variável impactou na construção de cada componente. Cada ponto do gráfico corresponde a uma linha de dado (dataset com 32 linhas no total).\n\n# Criando Biplot para PC1 e PC2\nhighcharter::hchart(mtcars.pca, choices = 1:2) %&gt;% \n  highcharter::hc_title(text = \"Biplot para PC1 e PC2\") %&gt;%\n  highcharter::hc_xAxis(title = list(text = \"PC1\")) %&gt;% \n  highcharter::hc_yAxis(title = list(text = \"PC2\"))\n\n\n\n\n\nPor fim, segue gráfico biplot comparando PC2 e PC3.\n\n# Criando Biplot para PC2 e PC3\nhighcharter::hchart(mtcars.pca, choices = 2:3) %&gt;% \n  highcharter::hc_title(text = \"Biplot para PC2 e PC3\") %&gt;%\n  highcharter::hc_xAxis(title = list(text = \"PC2\")) %&gt;% \n  highcharter::hc_yAxis(title = list(text = \"PC3\"))\n\n\n\n\n\n\n\n6.2.3 Considerações\nO resultado de uma análise de componentes principais pode ser utilizado para estimar outros modelos. Veja, se você possui um dataset com um número muito grande de variáveis, o PCA viabiliza que haja uma redução de muitas variáveis para poucos componentes principais que expressem a maior parte da variância dos dados. A maior dificuldade desta abordagem é analisar os resultados depois, pois não há uma interpretação “real” para os valores dos componentes principais.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#regressão-linear",
    "href": "machine_learning.html#regressão-linear",
    "title": "5  Machine Learning para negócios",
    "section": "7.1 Regressão linear",
    "text": "7.1 Regressão linear\nRelaciona uma variável dependente (target, rótulo) com uma matriz de variáveis explicativas (independentes):\n\\[y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\dots \\beta_n x_n + e\\]\nPossui diversas premissas acerca dos dados, como:\n\nRelação linear entre as variáveis.\nMédia e variância constantes (séries estacionárias).\nSem tendências ou sazonalidades.\n\nAlgumas considerações adicionais são:\n\nCaso as premissas não sejam verdade na prática (normalmente não é), as variáveis devem ser transformadas (ln, diferença, por exemplo).\nAs variáveis explicativas, se não forem numéricas, são transformadas em dummy (binarizadas).\nModelos de regressão são muito utilizados em análises de séries temporais.\n\nExemplos de usos:\n\nEstimar peso ou altura em determinada idade.\nDiversas aplicações em econometria em séries econômicas e financeiras.\nPrevisões em meteorologia.\n\nObs.: Métodos de regressão linear possuem diversas variações, e é uma área de estudo com muitos métodos específicos. O exemplo dado acima é apenas o caso mais simples.\n\n7.1.1 Exemplo: velocidade contra tempo para um carro parar (dados de 1920’s)\nVeja um exemplo de dados que mostram a velocidade de um carro e o tempo que ele levava até parar completamente.\nVariáveis:\n\nspeed: velocidade em milhas por hora (1 mph = 1,61 km/h).\ndist: distância que o carro levou para parar completamente, em pés (1 ft = 0,31 metros).\n\nHead dos dados:\n\nhead(cars,5)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n\n\nO modelo:\n\\[dist = \\alpha + \\beta_1 speed + e\\]\nEstimando o modelo:\n\n# Estimar o modelo\nreg_model &lt;- lm(dist ~ speed, cars)\n\nCriando gráfico de dispersão com a reta de melhor ajuste estimada pelo modelo:\n\n# Ajustar as informações do modelo para plotar no gráfico na sequência\nreg_model_df_chart &lt;- \n  augment(reg_model) %&gt;% \n  dplyr::mutate(.fitted = round(.fitted, 2))\n\n# Criando o gráfico\nhighcharter::highchart() %&gt;%\n  highcharter::hc_xAxis(title = list(text = \"Speed (mph)\")) %&gt;% \n  highcharter::hc_yAxis(title = list(text = \"Dist (ft)\")) %&gt;% \n  highcharter::hc_add_series(name = \"Valores reais\", \n                data = reg_model_df_chart, \n                type = \"scatter\", \n                hcaes(x = speed, y = dist)\n                ) %&gt;% \n  highcharter::hc_add_series(name = \"Predição\", \n                data = reg_model_df_chart, \n                type = \"line\", \n                hcaes(x = speed, y = .fitted)) %&gt;% \n  hc_title(text = \"Valores reais vs Preditos pela regressão linear\") %&gt;% \n  highcharter::hc_size(height = 400)\n\n\n\n\n\nPara concluir, veja que o gráfico de dispersão acima só é possível de ser realizado porque este exemplo trata de uma regressão simples, ou seja, em que temos apenas uma variável independente. Se tivéssemos um caso de regressão múltipla, com duas até \\(N\\) variáveis independentes, teríamos que fazer um gráfico de dimensão \\(N+1\\) para exemplificar a regressão linear, o que é possível. Por isso, a visualização da reta de melhor ajuste ao longo de todas as variáveis de uma regressão fica impossível a medida que mais variáveis independentes são adicionadas.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#regressão-logística",
    "href": "machine_learning.html#regressão-logística",
    "title": "5  Machine Learning para negócios",
    "section": "7.2 Regressão Logística",
    "text": "7.2 Regressão Logística\nA regressão logística é um tipo de regressão que tem como variável resposta (dependente) uma variável binária, ou seja, os valores possíveis são 0 ou 1, ocorrência ou não de um evento. O resultado do modelo é uma probabilidade, que pode ser transformado em labels de 0 e 1 ao levar em conta um ponto de corte na probabilidade estimada.\n\n“O modelo de regressão logística é adequado para trabalhar com dados qualitativos. Serve para as situações nas quais a variável dependente (Y) é binária (assume os valores de 0 e 1) e o valor que se busca é a probabilidade (\\(\\pi\\)) de que a Y seja 1 dado o valor de determinada variiável x, que poderá tanto ser binária, numérica ou dividida em categorias.” (CHATTERJEE; HADI, 2006; RYAN, 2009).\n\n\n7.2.1 Exemplo: regressão logística com dados do naufrágio do Titanic\nAnalisaremos um conjunto de dados dos passageiros do navio Titanic, que naufragou em 1912. Então aplicaremos um modelo de regressão logística para calcular o probabilidade de um passageiro, ao embarcar no navio, vir a sobreviver à tragédia (\\(y\\), variável dependente).\nO dicionário dos dados, que mostra o significado de cada variável, é o seguinte:\n\nsurvived: sobreviveu \\(= 1\\); não sobreviveu \\(=0\\);\npclass: classe (1, 2 e 3, indicando primeira, segunda e terceira classe);\nsex: masculino (male) e feminino (female);\nage: idade do passageiro;\nsibsp: número de irmãos e cônjuge (siblings and spouse) a bordo;\nparch: número de pais e filhos (parents and children) a bordo;\nfare: tarifa paga pelo passageiro.\n\nVeja como é o head dos dados (5 primeiras observações):\n\n# Download e pequeno tratamento dos dados\ndf_titanic &lt;- \n  readr::read_csv(\"https://gitlab.com/dados/open/raw/master/titanic.csv\") %&gt;% \n  dplyr::select(passengerid, survived, pclass, sex, age, sibsp, parch, fare) %&gt;% \n  dplyr::mutate_if(is.character, as.factor) %&gt;% \n  dplyr::mutate(survived = as.factor(survived)) %&gt;% \n  na.omit()\n\nRows: 1309 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, sex, ticket, cabin, embarked\ndbl (7): passengerid, survived, pclass, age, sibsp, parch, fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(df_titanic, 5)\n\n# A tibble: 5 × 8\n  passengerid survived pclass sex      age sibsp parch  fare\n        &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1           1 0             3 male      22     1     0  7.25\n2           2 1             1 female    38     1     0 71.3 \n3           3 1             3 female    26     0     0  7.92\n4           4 1             1 female    35     1     0 53.1 \n5           5 0             3 male      35     0     0  8.05\n\n\n\n\n7.2.2 Dividindo o dataset em treino e teste\nExistem diversas formas de fazer esta divisão pela linguagem R. Não há jeito certo ou errado, neste caso o importante é chegar ao objetivo de dividir o dataset.\n\n# O seed é um número que garante que a geração \"aleatória\" do computador será sempre a mesma. \n# Assim garanto que meu exemplo é reproduzível.\nset.seed(123)\n\n# Criar o subset de treino\ntrain &lt;- df_titanic %&gt;% dplyr::sample_frac(.70)\n\n# Criar o subset de teste com antijoin (pega tudo que não pertence)\ntest  &lt;- dplyr::anti_join(df_titanic, train, by = 'passengerid')\n\n\n\n7.2.3 Aplicando o modelo de regressão logística\nVamos utilizar a função glm(), nativa do R. Apenas certifique-se de configurar o parâmetro family para family = binomial(link = 'logit'). No termo da equação, a variável target é precedida de ~. Esta, inclusive, é uma terminologia padrão para a maior parte das funções de modelos supervisionados de machine learning no R.\nImportante: o modelo será aplicado no dataset de treino, e não no dataset completo.\n\n# Rodando o modelo\nfit &lt;- \n  glm(\n    survived ~ pclass + sex + age + sibsp + parch + fare,\n    family = binomial(link = 'logit'),\n    data = train\n  )\n\n# Vendo como ficou o modelo\nfit\n\n\nCall:  glm(formula = survived ~ pclass + sex + age + sibsp + parch + \n    fare, family = binomial(link = \"logit\"), data = train)\n\nCoefficients:\n(Intercept)       pclass      sexmale          age        sibsp        parch  \n   5.059665    -1.029637    -3.725177    -0.034072    -0.222305    -0.135509  \n       fare  \n   0.002101  \n\nDegrees of Freedom: 731 Total (i.e. Null);  725 Residual\nNull Deviance:      983 \nResidual Deviance: 543.8    AIC: 557.8\n\n\nO resultado acima mostra os coeficientes estimados pelo modelo. Estes coeficientes compõem a equação da regressão logística.\nCada coeficiente tem um p-valor associado, que mostra se o impacto da variável associada ao coeficiente foi significativo estatisticamente no modelo. Veja a seguir:\n\nbroom::tidy(fit) %&gt;% \n  dplyr::mutate(p.value = round(p.value,4))\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  5.06      0.643       7.87   0     \n2 pclass      -1.03      0.177      -5.83   0     \n3 sexmale     -3.73      0.248     -15.0    0     \n4 age         -0.0341    0.00881    -3.87   0.0001\n5 sibsp       -0.222     0.136      -1.63   0.103 \n6 parch       -0.136     0.128      -1.06   0.289 \n7 fare         0.00210   0.00220     0.954  0.340 \n\n\n\n\n7.2.4 Realizando as predições\nUsando a função predict do R para fazer as predições no dataset de teste com base no modelo que foi treinado.\n\n# Fazendo as predições\npred &lt;- predict(fit, newdata = test, type = \"response\")\n\nNa sequência veja como fica a tabela unificando a coluna de predição com os valores reais da variável dependente.\n\n# Concatenando a coluna predita com o dataset de teste\nDT::datatable(\n  cbind(\"pred\" = round(pred, 4), test)\n)\n\n\n\n\n\n\n\n7.2.5 Avaliação do modelo\nA seguir, vamos criar a matriz confusão e calcular a acurácia “na mão”, sem usar função específica para isso, a fim de entendermos o procedimento.\nNeste caso, vale ressaltar que o threshold (ponto de corte na probabilidade para definir se a saída será 0 ou 1) é definido de forma arbitrária. Porém, é possível aplicar métodos de otimização do threshold. Um exemplo de uso para otimização é a função thresholder() do pacote caret. Veja mais. Esta função não foi aplicada neste exemplo porque todo o modelo deve ter sido rodado com funções do caret para que seja possível realizar esta otimização.\nPrimeiro, a matriz confusão:\n\n# Definindo o ponto de corte (cut-off ou threshold)\nthreshold &lt;- .5\n\n# Transformando predições em binários\npred_binario &lt;- ifelse(pred &gt; threshold,1,0)\n\n# Criando a matrix\nconfusion_matrix &lt;- table(data.frame(pred_binario, test$survived) )\nconfusion_matrix\n\n            test.survived\npred_binario   0   1\n           0 172  34\n           1  14  93\n\n\nAgora, a acurácia:\n\n# Calculando acurácia\nacc &lt;- sum(diag(confusion_matrix))/sum(confusion_matrix)\n\n# Arredondando e colocando em percentual\nacc &lt;- round(acc*100, digits = 2)\n\n# Mostrar objeto\nacc\n\n[1] 84.66\n\n\nPorém, podemos também utilizar o pacote caret, obter mais métricas e calcular de forma mais rápida.\n\n# Unificando as predições com o valor real\npred_tbl &lt;- data.frame(as.factor(pred_binario), as.factor(as.numeric(test$survived)-1))\ncolnames(pred_tbl) &lt;- c(\"pred\", \"survived\")\n\n# Model metric\ncaret::confusionMatrix(pred_tbl$pred, pred_tbl$survived )\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 172  34\n         1  14  93\n                                          \n               Accuracy : 0.8466          \n                 95% CI : (0.8019, 0.8847)\n    No Information Rate : 0.5942          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.6738          \n                                          \n Mcnemar's Test P-Value : 0.006099        \n                                          \n            Sensitivity : 0.9247          \n            Specificity : 0.7323          \n         Pos Pred Value : 0.8350          \n         Neg Pred Value : 0.8692          \n             Prevalence : 0.5942          \n         Detection Rate : 0.5495          \n   Detection Prevalence : 0.6581          \n      Balanced Accuracy : 0.8285          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nExercício::\n\nAltere a variávei threshold para outros valores entre 0 e 1 e rode o código novamente até as métricas pela função confusionMatrix do pacote caret. Veja como as me’tricas se comportam.\n\n\n\n7.2.6 ROC Curve\nVeja a curva ROC do modelo estimado.\n\n# Carregando pacote para plotar curva ROC\nlibrary(pROC)\n\n# Organizando a tabela de dados para calcular as métricas da curva ROC\npred_roc &lt;- \n  dplyr::tibble(\n    pred,\n    \"survived\" = as.factor(as.numeric(test$survived)-1)\n  ) %&gt;% arrange(desc(pred))\n\n# Criando objeto com as métricas para curva ROC\nroc &lt;- pROC::roc(pred_roc$survived , pred_roc$pred)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nroc\n\n\nCall:\nroc.default(response = pred_roc$survived, predictor = pred_roc$pred)\n\nData: pred_roc$pred in 186 controls (pred_roc$survived 0) &lt; 127 cases (pred_roc$survived 1).\nArea under the curve: 0.8876\n\n# Se desejar, é possível (e bem simples) utilizar o próprio pacote pROC para plotar a curva ROC.\n#plot.roc(roc)\n\n\n# Criando função para fazer gráficos da curva ROC\nhchart.roc &lt;- function(x){\n  dplyr::tibble(tpr = round(x$sensitivities*100,2),\n              fpr = round((1-x$specificities)*100,2)) %&gt;%\n  highcharter::hchart(\n    name = \"TPR (%)\",\n    type = \"area\",\n    hcaes(x = \"fpr\", y = \"tpr\"),\n    showInLegend = FALSE\n  ) %&gt;%\n  highcharter::hc_xAxis(title = list(text = \"Taxa de Falso Positivo (1- Especificidade)\"), min = 0, max = 100) %&gt;%\n  highcharter::hc_yAxis(title = list(text = \"Taxa de Verdadeiro Positivo (Sensibilidade)\"), min = 0, max = 100) %&gt;%\n  highcharter::hc_title(text = \"Curva Característica de Operação Relativa (ROC)\") %&gt;%\n  highcharter::hc_colors(\"#aaa\") %&gt;% \n  highcharter::hc_size(height = 380)\n}\n\n# Aplicando a função criada e plotando gráfico.\nhchart.roc(roc)\n\n\n\n\n\nPor fim, a estimação da AUC (Area Under the Curve):\n\npROC::auc(roc)\n\nArea under the curve: 0.8876",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#árvore-de-decisão",
    "href": "machine_learning.html#árvore-de-decisão",
    "title": "5  Machine Learning para negócios",
    "section": "7.3 Árvore de Decisão",
    "text": "7.3 Árvore de Decisão\nEntre os modelos de aprendizado de máquina mais comuns e mais úteis estão as árvores de decisão. Elas são tão utilizadas porque fornecem ao usuário final uma fácil interpretação e desenham uma espécie de caminho a ser percorrido para alcançar um determinado objetivo.\n\nÁrvores de decisão são fáceis de interpertar, pois fornecem as regras de negócio necessárias para classificar os dados e fornecer uma predição.\n\nAlgumas das características são:\n\nComposta por nós, em que os extremos são:\n\nraiz (primeiro) e\nfolhas (últimos, onde o resultado final de um conjunto de decisões é mostrado).\n\n\nVeja graficamente como fica esta estrutura:\n\nComo é definido o que entra em cada nó?\n\nCada nó é o resultado de sucessivas subdivisões nos dados, criando subconjuntos cada vez mais puros.\nUm subconjunto será mais puro na medida que conter menos classes (ou apenas uma) da variável resposta (target).\n\nO critério utilizado para realizar as partições é o da utilidade do atributo para a classificação. Aplica-se, por este critério, um determinado ganho de informação a cada atributo.\nA pureza dos subconjuntos (nós) é definida pelo ganho de informação. Para calcular este ganho de informação utiliza-se geralmente a entropia ou o índice de Gini.\nA entropia é uma medida de surpresa. A entropia vai de 0 a 1, e se ela for zero, então não há supresa nas respostas possíveis. Entenda melhor pelo diagrama a seguir.\n\nO índice Gini (veja mais) mede o grau de heterogeneidade dos dados. Logo, pode ser utilizado para medir a impureza de um nó. Mas é também um índice usado em diversos campos, como por exemplo para medir a desigualdade de renda nos países (veja).\nO índice é calculado por nó, e quando é igual a zero, o nó é puro. Por outro lado, quando ele se aproxima do valor um, o nó é impuro (aumenta o número de classes uniformemente distribuídas neste nó).\nAlguns algoritmos de árvores de decisão utilizam entropia, outros o índice de Gini. Porém, geralmente a performance do algoritmo não se altera entre usar um método ou outro. Por usar logaritmo em sua formulação, a entropia costuma gerar um maior custo computacional.\n\nÁrvores de decisão e overfitting\nProblemas de overfitting ocorrem quando um modelo explica situações extremamente específicas, que constam no dataset de treino, mas não ocorrerão na prática novamente.\nÁrvores de decisão podem ficar muito copmlexas e extensas, caindo no problema de overfitting.\nNestes casos, uma saída é aplicar técnicas de poda da árvore. No R é possível utilizar a função prune() do pacote rpart.\n\n\n7.3.1 Lendo o conjunto de dados do Titanic\nVamos buscar o arquivo csv do repositório. Após iremos excluir as variáveis a não serem utilizadas e separar o dataset em treino e teste.\n\ndf_titanic_tree &lt;- \n  read_csv(\"https://gitlab.com/dados/open/raw/master/titanic.csv\") %&gt;% \n  mutate_if(is.character, as.factor) %&gt;% \n  select(-name, -ticket, -embarked, -cabin, - passengerid) %&gt;% \n  mutate(survived = as.factor(survived))\n\nRows: 1309 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, sex, ticket, cabin, embarked\ndbl (7): passengerid, survived, pclass, age, sibsp, parch, fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_titanic_tree %&gt;% \n  head(5) \n\n# A tibble: 5 × 7\n  survived pclass sex      age sibsp parch  fare\n  &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0             3 male      22     1     0  7.25\n2 1             1 female    38     1     0 71.3 \n3 1             3 female    26     0     0  7.92\n4 1             1 female    35     1     0 53.1 \n5 0             3 male      35     0     0  8.05\n\n\nNa sequência segue o dicionário dos dados:\n\nsurvived: sobreviveu \\(= 1\\); não sobreviveu \\(=0\\);\npclass: classe (1, 2 e 3, indicando primeira, segunda e terceira classe);\nsex: masculino (male) e feminino (female);\nage: idade do passageiro;\nsibsp: número de irmãos e cônjuge (siblings and spouse) a bordo;\nparch: número de pais e filhos (parents and children) a bordo;\nfare: tarifa paga pelo passageiro.\n\n\n\n7.3.2 Criando uma árvore de decisão com todo o dataset\nPrimeiro, apenas para fins didáticos, vamos contruir uma árvore de decisão para todo o dataset, ou seja, não iremos considerar o processo de machine learning de treinar e testar o modelo.\nNa árvore a seguir, cada nós mostra: - a predição binária da classe (não sobreviveu = 0; sobreviveu = 1); - a probabilidade predita de sobrevivência (de 0 a 1); - a porcentagem de observações no nós.\n\nrtree_fit &lt;-\n  rpart(formula = survived ~ ., \n        data = df_titanic_tree,\n        parms = list(split = \"information\") # Usar \"information\" para entropia ou \"gini\" para índice de Gini.\n        )\n\nrpart.plot(rtree_fit)\n\n\n\n\n\n\n\n\n\n\n7.3.3 Divindo em treino e teste\nAgora sim, iremos dividir o conjunto de dados em treino e teste.\n\n# Separar os dados em treino e teste\nset.seed(100)\ndata &lt;- c(\"training\", \"test\") %&gt;%\n  sample(nrow(df_titanic_tree), replace = T) %&gt;%\n  split(df_titanic_tree, .)\n\nVamos agora criar a árvore de decisão nos dados de treino:\n\n# Criar a árvore de decisão\nrtree_fit &lt;- \n  rpart(survived ~ ., \n          data$training,\n          parms = list(split = \"information\")\n          )\nrpart.plot(rtree_fit)\n\n\n\n\n\n\n\n\nExercício. Troque o seed para outros valores aleatórios e verifique como a estrutura da árvore muda.\n\n\n7.3.4 Fazendo as predições\nAplicando a função preditc().\n\npred_num &lt;- predict(rtree_fit,  newdata = data$test)[,2] # segunda coluna para pegar apenas survived = 1\npred &lt;- predict(rtree_fit,  newdata = data$test, type = \"class\")\n\nMatriz confusão:\n\ntable(pred, data$test$survived )\n\n    \npred   0   1\n   0 370  48\n   1  50 205\n\n\nPelo pacote caret:\n\ncaret::confusionMatrix(pred, data$test$survived)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 370  48\n         1  50 205\n                                          \n               Accuracy : 0.8544          \n                 95% CI : (0.8254, 0.8802)\n    No Information Rate : 0.6241          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6901          \n                                          \n Mcnemar's Test P-Value : 0.9195          \n                                          \n            Sensitivity : 0.8810          \n            Specificity : 0.8103          \n         Pos Pred Value : 0.8852          \n         Neg Pred Value : 0.8039          \n             Prevalence : 0.6241          \n         Detection Rate : 0.5498          \n   Detection Prevalence : 0.6211          \n      Balanced Accuracy : 0.8456          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n7.3.5 ROC Curve\nVeja a curva ROC do modelo estimado.\n\n#library(pROC)\n\npred_roc &lt;- data.frame(pred_num, as.factor(as.numeric(data$test$survived)-1)) %&gt;% arrange(desc(pred_num))\ncolnames(pred_roc) &lt;- c(\"pred\", \"survived\")\n\nroc &lt;- roc(pred_roc$survived , pred_roc$pred)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n#plot.roc(roc)\nhchart.roc(roc)\n\n\n\n\n\n\n\n7.3.6 Fontes\n\nDetalhes sobre árvores de decisão no R:\n\nhttp://www.milbo.org/doc/prp.pdf",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#random-forest",
    "href": "machine_learning.html#random-forest",
    "title": "5  Machine Learning para negócios",
    "section": "7.4 Random Forest",
    "text": "7.4 Random Forest\nUm dos pontos negativos das árvores de decisão é a variância nos seus resultados. Duas árvores treinadas no mesmo conjunto podem apresentar resultados bastante distintos. Com isso, uma random forest (ou floresta aleatória) é um grande número de árvores de decisão rodadas em paralelo.\nCom este grande número de árvores cria-se uma espécie de “comitês” em que os caminhos mais votados pelas diversas árvores tornam-se o resultado do modelo de random forest. Por esta característica pode-se dizer que random forest é um tipo de algoritmo de ensemble learning.\n\nRandom forests, ou florestas aleatórias, representam múltiplas árvores de decisão que são rodadas e unificadas, com o fim obter uma maior acurária e predições mais assertivas.\n\nO procedimento adotado para realizar estes votos é chamado de bagging. A ideia essencial do bagging é obter a média de diversas árvores descorrelacionadas e com ruído. É simples de treinar e possui parâmetros fáceis para ajustes, o que resultou na grande popularidade deste tipo de modelo.\n\nEnsemble learning, em geral, trata de um modelo que faz predições baseadas em um número de modelos diferentes. Ao combinar modelos individuais, o modelo ensemble tende a ser mais flexível (com menos viés) e menos data-sensitive (com menos variância).  Os dois mais populares métodos de ensemble são bagging e boosting. - Bagging: treina um grupo de modelos individuais em paralelo. Cada modelo é treinado com base em subsets aleatórios dos dados. - Boosting: treina um grupo de modelos individuais de forma sequencial. Cada modelo é treinado de forma a aprender com os erros feitos pelo modelo anterior. Fonte.\n\n\n7.4.1 Exemplo do uso de random forest para prever a saída de colaboradores\nCarregando os pacotes básicos:\n\n# Pacotes a serem utilizados\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ranger)\nlibrary(ggplot2)\n\nOs dados utilizados neste exemplo refletem características dos colaboradores de uma empresa. São dados fictícios criados por cientistas de dados da IBM com o fim de alimentar simulações no Watson (ferramenta de analytics da IBM). Veja aqui o link do dataset original.\nA seguir os dados são importados. Veja no comando select as variáveis do conjunto de dados que serão utilizadas.\n\ndf_hr_turnover &lt;-\n  read_csv(\"https://gitlab.com/dados/open/raw/master/ibm_hr_emplyee_attrition.csv\") %&gt;%\n  mutate_if(is.character, as.factor) %&gt;%\n  select(\n    \"Attrition\",\n    \"Age\",\n    \"Department\",\n    \"DistanceFromHome\",\n    \"Education\",\n    \"EducationField\",\n    \"EmployeeCount\",\n    \"EnvironmentSatisfaction\",\n    \"Gender\",\n    \"HourlyRate\",\n    \"JobInvolvement\",\n    \"JobLevel\",\n    \"JobRole\",\n    \"JobSatisfaction\",\n    \"MaritalStatus\",\n    \"MonthlyIncome\",\n    \"MonthlyRate\",\n    \"NumCompaniesWorked\",\n    \"Over18\",\n    \"OverTime\",\n    \"PercentSalaryHike\",\n    \"PerformanceRating\" ,\n    \"RelationshipSatisfaction\",\n    \"StandardHours\",\n    \"StockOptionLevel\",\n    \"TotalWorkingYears\",\n    \"TrainingTimesLastYear\",\n    \"WorkLifeBalance\",\n    \"YearsAtCompany\",\n    \"YearsInCurrentRole\",\n    \"YearsSinceLastPromotion\",\n    \"YearsWithCurrManager\"\n  )\n\nRows: 1470 Columns: 35\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\ndbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n7.4.2 Dividindo em treino e teste\nAqui faremos a divisão do conjunto de dados entre treino e teste.\n\n## set the seed to make your partition reproducible\nset.seed(123)\n\n# Create training subset\ntrain &lt;- df_hr_turnover %&gt;% dplyr::sample_frac(.70)\n# Create test subset\ntest  &lt;- dplyr::anti_join(df_hr_turnover, train)\n\nJoining with `by = join_by(Attrition, Age, Department, DistanceFromHome,\nEducation, EducationField, EmployeeCount, EnvironmentSatisfaction, Gender,\nHourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus,\nMonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime,\nPercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours,\nStockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance,\nYearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion,\nYearsWithCurrManager)`\n\n\n\n\n7.4.3 Modelando as random forests\nIremos utilizar o pacote ranger, da linguagem R. Este pacote é amplamente utilizado e muito bem implementado. A documentação é toda embasada em artigos científicos, o que dá maior credbilidade às funções utilizadas.\nOs parâmetros que iremos utilizar são:\n\nformula: fórmula do modelo, no padrão \\(var_{target} \\text{ ~ } var_1 + var_2 + ... + var_n\\) ;\ndata: objeto com o conjunto de dados;\nmax.depth: profundidade de nós máxima para cada árvore;\nnum.trees: número de árvores na floresta;\nprobability: indica se o modelo irá gerar um resultado probabilístico ou com as labels da variável target.\nimportance : forma de cálculo da importância das variáveis. Exemplo: “impurity” representa o índice de Gini utilizado para classificação.\n\nAjuste do modelo\n\n# Ajustar e mostrar o modelo Random Forest\n#####\n\n# Se desejar escolher certas variáveis\n # fml &lt;- \"Attrition ~ OverTime + MonthlyIncome + Age + TotalWorkingYears + DistanceFromHome\"\n\n# Se desejar incluir todas variáveis do dataset\nfml &lt;- \"Attrition ~ .\"\n\n# Definir o número de árvores que o modelo irá rodar\nntrees &lt;- 100\n\n# Estimando modelo com resultado como labels\nfit_turnover_rf_class &lt;-\n  ranger::ranger(\n    formula = fml,\n    data = train,\n    max.depth = 15,\n    num.trees = ntrees,\n    probability = FALSE,\n    importance = \"impurity\", # \"impurity\" é o coeficiente de Gini\n    seed = 999\n  )\n\n# Estimando modelo probabilístico\nfit_turnover_rf_prob &lt;-\n  ranger::ranger(\n    formula = fml,\n    data = train,\n    max.depth = 15,\n    num.trees = ntrees,\n    probability = TRUE,\n    importance = \"impurity\", # \"impurity\" é o coeficiente de Gini\n    seed = 999\n  )\n\n\n\n7.4.4 Importância das variáveis\nNo gráfico a seguir as variáveis mais relevantes para realizar as classificações são elencadas. No caso de árvores de decisão (e também random forests) o gráfico mostra as variáveis ordenadas por nível de ganho informacional gerado após os splits nos dado (que geram os nós das árvores).\n\nggplot(\n  stack(fit_turnover_rf_prob$variable.importance) %&gt;% \n    arrange(desc(values)), \n  aes(x=reorder(ind,values), y=values,fill=values)) +\n   geom_bar(stat=\"identity\", position=\"dodge\")+ coord_flip()+\n   ylab(\"Importância das variáveis\")+\n   xlab(\"Variáveis\")+\n   ggtitle(\"Informação de cada variável\")+\n   guides(fill=F)+\n   scale_fill_gradient(low=\"red\", high=\"blue\")\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\n\nCuriosidade sobre a importância das variáveis e Random Forest\nUm das áreas de machine learning é feature selection, que trata de selecionar as variáveis que são mais relevantes em um modelo, excluindo variáveis que mais trazem ruídos do que ajudam nas predições.\nExistem várias formas de realizar feature selection, como utilizar análise de correlação, por exemplo. Mas há outras formas, como utilizar Random Forest e os métodos de cálculos de importância das variáveis para filtrar apenas as variáveis mais relevantes.\nAlém do método mostrado anteriormente, o pacote ranger permite também identificar o p-valor da importância das variáveis, o que viabiliza ter um ponto de corte, como eliminar todas as variáveis com p-valor maior que \\(0.10\\), por exemplo. Veja:\n\nVariáveis escolhidas:\n\n\n# Rodando novamente o modelo com o parâmetro \"importance\" em impurity_corrected para viabilizar análise de p-valor\nfit_turnover_rf_prob &lt;-\n  ranger::ranger(\n    formula = fml,\n    data = train,\n    max.depth = 15,\n    num.trees = ntrees,\n    probability = TRUE,\n    importance = \"impurity_corrected\", # \"impurity\" é o coeficiente de Gini\n    seed = 999\n  )\n\nranger::importance_pvalues(fit_turnover_rf_prob) %&gt;% \n  dplyr::as_tibble(rownames = \"Variável\") %&gt;% \n  dplyr::arrange(pvalue) %&gt;% \n  dplyr::filter(pvalue &lt;= 0.1)\n\nWarning in ranger::importance_pvalues(fit_turnover_rf_prob): This method is\ntested for classification only, use with care.\n\n\nWarning in ranger::importance_pvalues(fit_turnover_rf_prob): Only few negative\nimportance values found, inaccurate p-values. Consider the 'altmann' approach.\n\n\n# A tibble: 18 × 3\n   Variável                 importance pvalue\n   &lt;chr&gt;                         &lt;dbl&gt;  &lt;dbl&gt;\n 1 Age                           4.32  0     \n 2 EducationField                2.05  0     \n 3 JobLevel                      1.98  0     \n 4 JobRole                       2.40  0     \n 5 MaritalStatus                 2.59  0     \n 6 MonthlyIncome                 4.83  0     \n 7 OverTime                      8.08  0     \n 8 StockOptionLevel              3.62  0     \n 9 TotalWorkingYears             2.85  0     \n10 YearsAtCompany                2.89  0     \n11 Department                    0.643 0.0769\n12 EnvironmentSatisfaction       0.998 0.0769\n13 JobInvolvement                0.481 0.0769\n14 JobSatisfaction               1.72  0.0769\n15 NumCompaniesWorked            0.611 0.0769\n16 RelationshipSatisfaction      0.483 0.0769\n17 WorkLifeBalance               1.64  0.0769\n18 YearsWithCurrManager          1.61  0.0769\n\n\n\nVariáveis eliminadas:\n\n\nranger::importance_pvalues(fit_turnover_rf_prob) %&gt;% \n  dplyr::as_tibble(rownames = \"Variável\") %&gt;% \n  dplyr::arrange(pvalue) %&gt;% \n  dplyr::filter(pvalue &gt; 0.1)\n\nWarning in ranger::importance_pvalues(fit_turnover_rf_prob): This method is\ntested for classification only, use with care.\n\n\nWarning in ranger::importance_pvalues(fit_turnover_rf_prob): Only few negative\nimportance values found, inaccurate p-values. Consider the 'altmann' approach.\n\n\n# A tibble: 13 × 3\n   Variável                importance pvalue\n   &lt;chr&gt;                        &lt;dbl&gt;  &lt;dbl&gt;\n 1 TrainingTimesLastYear       0.392   0.154\n 2 YearsInCurrentRole          0.224   0.231\n 3 Education                   0.142   0.308\n 4 YearsSinceLastPromotion     0.178   0.308\n 5 PerformanceRating           0.0269  0.385\n 6 EmployeeCount               0       0.615\n 7 Over18                      0       0.615\n 8 StandardHours               0       0.615\n 9 MonthlyRate                -0.128   0.692\n10 Gender                     -0.189   0.769\n11 DistanceFromHome           -0.326   0.846\n12 PercentSalaryHike          -0.425   0.923\n13 HourlyRate                 -1.90    1    \n\n\n\n\n\n7.4.5 Avaliando a floresta alatória\nAplicando a função preditc().\n\n# Para confusion matrix\npred_class &lt;- \n  predict(\n  fit_turnover_rf_class,\n  test)\n\n# Para curva ROC\npred_prob &lt;- \n  predict(\n  fit_turnover_rf_prob,\n  test)\n\nWarning in predict.ranger(fit_turnover_rf_prob, test): Forest was grown with\n'impurity_corrected' variable importance. For prediction it is advised to grow\nanother forest without this importance setting.\n\n\nA matriz confusão:\n\nlibrary(caret)\n\nconfusionMatrix(pred_class$predictions, \n                test$Attrition)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  362  66\n       Yes   4   9\n                                          \n               Accuracy : 0.8413          \n                 95% CI : (0.8038, 0.8741)\n    No Information Rate : 0.8299          \n    P-Value [Acc &gt; NIR] : 0.2875          \n                                          \n                  Kappa : 0.1625          \n                                          \n Mcnemar's Test P-Value : 3.079e-13       \n                                          \n            Sensitivity : 0.9891          \n            Specificity : 0.1200          \n         Pos Pred Value : 0.8458          \n         Neg Pred Value : 0.6923          \n             Prevalence : 0.8299          \n         Detection Rate : 0.8209          \n   Detection Prevalence : 0.9705          \n      Balanced Accuracy : 0.5545          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\n7.4.6 Considerações\nEste modelo apresenta um problema: uma alta taxa de falso negativo. Ou seja, para um número considerável de colaboradores foi predito que não sairíam da empresa, mas saíram. Além disso, neste caso o custo de um falso negativo é alto!\n\n\n7.4.7 Fontes\n\nDetalhes sobre random forest:\n\nhttps://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd\n\nConjunto de dados:\n\nhttps://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n\nOutro exemplo de análise com o mesmo conjunto de dados:\n\nhttps://rpubs.com/ssindw/295564",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#redes-neurais-artificiais-e-deep-learning",
    "href": "machine_learning.html#redes-neurais-artificiais-e-deep-learning",
    "title": "5  Machine Learning para negócios",
    "section": "7.5 Redes Neurais Artificiais e Deep Learning",
    "text": "7.5 Redes Neurais Artificiais e Deep Learning\nDeep learning é uma forma de aplicação de redes neurais artificiais (RNA), em que há mais de uma camada (geralmente são inúmeras) entre a camada de entrada (input) e a camada de saída (output). É também chamada de Deep Neural Network.\nUma rede neural é “mais profunda” dependendo da quantidade de camadas que possui. Estas camadas são chamadas de hidden layers. Quanto maior o número de hidden layers mais “black-box” fica o modelo, se tornando praticamente impossível dar uma explicação “real” aos parâmetros que o modelo define ao longo das entradas e saídas das diversas camadas.\nVale ainda ressaltar, conforme será explicado na sequência, que a complexidade de uma rede neural não é devida somente à quantidade de camadas de um modelo, mas também está relacionada ao montante de neurônios que estarão contidos em cada camada.\nSendo assim, trabalhar com deep learning é uma forma de utilizar o elevado potencial computacional que temos atualmente para executar modelos extremamente complexos em busca de predições melhores.\nNa sequência, entenda como funcionam as redes neurais, partindo desde o caso mais simples, sem nenhuma hidden layer, até casos complexos, em que há inúmeras camadas, que podemos chamar de “deep learning”.\n\nConceito de Redes Neurais Artificiais\nRedes neurais executam seu processamento por meio de neurônios distribuídos em camadas, que podem ser de entrada, ocultas ou de saída.\n\nCamada de entrada: input dos dados;\nCamadas ocultas (hidden layers): processamento;\nCamada de saída: é o resultado final do processamento de uma rede neural.\n\nPara que haja o processamento, a rede precisa de uma função de ativação, que é a função matemática utilizada para definir os pesos associados a cada rede. Algumas funções que podem ser utilizadas são: linear, logística (sigmoide), rectified linear units, softmax, etc. A função logística foi a mais usada por muito tempo, mas com o surgimento de cada vez mais aplicações para o uso de redes neurais, algumas funções específicas se tornaram mais assertivas ou mais eficientes dependendo do caso. Na sequência, serão apresentados mais detalhes sobre funções de ativação em deep learning.\nVeja a seguir um exemplo de como podemos demonstrar graficamente uma rede neural.\n\nExemplo: classificar o colaborador entre departamento de TI e RH.\n\nPara esta tarefa podemos construir a rede neural da figura ao lado, a qual contém as seguintes características:\n\nCamada de entrada: 4 sinais binários (representados pelos círculos) indicando formação do colaborador (ciência da computação ou psicologia) e gênero (masculino ou feminino).\nCamada oculta com 3 neurônios (definidos pelo usuário).\nCamada de saída: duas camadas, indicando departamento do colaborador se TI (sim ou não) ou RH (sim ou não).\n\nVale ressaltar que não há programação, a priori, nos neurônios. Eles trabalham com tentativa e erro atrelados a um sistema de feedback, que atribui pesos maiores para as redes mais assertivas.\nAlém disso, geralmente os modelos de redes neurais criam estes sinais binários automaticamente a partir de um input de variáveis categóricas. Ou seja, na prática não precisaríamos ter 4 variáveis binárias em um conjunto de dados para compor os dados de entrada, pois bastariam duas variáveis categóricas identificando o departamento e o gênero.\nVale lembrar que variáveis numéricas também podem servir de input nas redes neurais, sendo que estas são tratadas em sua forma original.\n\n\n\n\n\n\n\n\n\nComo Funcionam as Redes Neurais?\nApós entender a lógica das redes neurais, vamos agora avançar desde o caso mais simples de rede neural até chegar em casos complexos, que é onde entra o conceito de deep learning.\nA rede neural mais simples é chamada de perceptron, e contém:\n\n1 ou mais inputs;\n1 processador;\n1 única saída.\n\nUm perceptron segue 4 passos principais:\n\nRecebe os inputs;\nDá peso aos inputs;\nSoma os inputs;\nGera um output.\n\nA seguir é apresentado um exemplo de uma tabela de dados muito simples para aplicarmos uma rede neural. O objetivo com este exemplo é demostrar como funcionam as redes neurais e também como é possível construir modelos extremamente complexos mesmo com uma tabela de dados com poucas linhas e colunas.\nA tabela possui a seguinte estrutura:\n\nDuas variáveis explicativas: \\(Talento\\) (1 = sim, 0 = não) e \\(Sem~gest.~conseq.\\) (1 = mais de 2 anos sem gestão de consequência, 0 = teve gestão de consequência nos últimos 2 anos).\nUma variável dependente: \\(Turnover\\) (1 = colaborador pediu demissão, 0 = colaborador ativo).\n\nNeste exemplo, que foca na simplicidade para demonstrar o funcionamento das redes neurais, a tabela foi construída usando o operador lógico “E”.\n\n# Criando Y, a variável dependente\nturnover &lt;- c(rep(0, 3), 1)\n\n# Criando o conjunto de dados fictício\nbinary.data &lt;- \n  data.frame(expand.grid(\"talento\" = c(0, 1), \"sem.gest.conseq\" = c(0, 1)), turnover) %&gt;% as_tibble()\n# Obs.: Se adicionar Y2 = c(0,1,0,0) e usar a formula Y + Y2 ~ Var1 + Var2 é possível ter 2 saídas\n\nbinary.data %&gt;%\n  dplyr::rename(\"Talento\" = talento, \n                \"Sem gestão de conseq.\" = sem.gest.conseq, \n                \"Turnover\" = turnover ) \n\n# A tibble: 4 × 3\n  Talento `Sem gestão de conseq.` Turnover\n    &lt;dbl&gt;                   &lt;dbl&gt;    &lt;dbl&gt;\n1       0                       0        0\n2       1                       0        0\n3       0                       1        0\n4       1                       1        1\n\n\nCom estes dados podemos construir um exemplo de perceptron, conforme segue:\n\nlibrary(neuralnet)\n\n# Seed para manter o mesmo modelo quando replicar\nset.seed(123)\n\n# Criando o modelo\nnet0hidden &lt;- neuralnet(\n  turnover ~ talento + sem.gest.conseq,\n  binary.data,\n  hidden = 0,\n  err.fct = \"ce\",\n  linear.output = FALSE\n)\n\n# Criando gráfico\nplot(net0hidden, rep = \"best\", information = FALSE)\n\n\n\n\n\n\n\n\nO gráfico acima mostra:\n\nUma input layer (círculos da esquerda);\nUma output layer (círculo da direita).\n\nAlém disso:\n\nOs números nas linhas apontam os melhores pesos.\nO anexo em azul representa um bias node, que nada mais é que uma constante aplicada.\n\nNeste exemplo foi utilizada uma função de ativação logística, conforme segue:\n\\[z = -11.86 + 7.7538 ~ talento + 7.7552 ~ sem~gest.~conseq.~ \\]\nsendo que \\(z\\) é chamado de “ativador”.\nO ativador passa por uma função sigmoide (neste caso, mas poderia ser outro tipo de função), assim como ocorre em uma regressão logística, gerando:\n\\[ turnover = \\frac{1}{1 + e^{−(-11.86 + 7.7538 ~ talento + 7.7552 ~ sem~gest.~conseq.)}} \\]\nPara ter os resultados da predição do perceptron exemplificado basta substituir os valores das variáveis talento e sem gest. conseq. para 0 ou 1 e resolver a equação apresentada acima para todas combinações possíveis. Veja como fica o resultado:\n\n# Criando a função logística\nY = function(Var1,Var2){\n  1/(1+exp(-(-11.86048+7.75382*Var1+7.75519*Var2)))\n}\n\n# Obtendo as probabilidades para as situações possíveis\nbinary.data %&gt;%\n  dplyr::mutate(predicao = round(c(Y(0, 0),\n                            Y(1, 0),\n                            Y(0, 1),\n                            Y(1, 1)), 3)) %&gt;% \n  dplyr::rename(\"Talento\" = talento, \n                \"Sem gestão de conseq.\" = sem.gest.conseq, \n                \"Turnover\" = turnover,\n                \"Predição\" = predicao)\n\n# A tibble: 4 × 4\n  Talento `Sem gestão de conseq.` Turnover Predição\n    &lt;dbl&gt;                   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1       0                       0        0    0    \n2       1                       0        0    0.016\n3       0                       1        0    0.016\n4       1                       1        1    0.975\n\n\nA generalização de perceptrons é chamada de neurônios. São então criadas camadas (hidden layers), em que o output de um neurônio serve de input para um neurônio da próxima camada, formando assim a rede neural.\nExemplo de rede neural com uma hidden layer de 1 neurônio:\n\n# Seed para replicação\nset.seed(123)\n\n# Criando modelo com 1 neurônio em uma hidden layer\nnet1neur &lt;- \n  neuralnet(\n    turnover ~ talento + sem.gest.conseq,\n    binary.data,\n    hidden = 1,\n    err.fct = \"ce\",\n    linear.output = FALSE\n  )\n\n# Gerando gráfico da rede\nplot(\n  net1neur,\n  rep = \"best\",\n  information = FALSE\n)\n\n\n\n\n\n\n\n\nComo exemplo, veja como ficaria se aumentássemos o número de neurônios para 4, mas ainda com apenas uma hidden layer:\n\n# Seed para replicação\nset.seed(123)\n\n# Criando modelo com 4 neurônios em uma hidden layer\nnet4neur &lt;-\n  neuralnet(\n    turnover ~ talento + sem.gest.conseq,\n    binary.data,\n    hidden = 4, # Número de neurônios na hidden layer\n    err.fct = \"ce\",\n    linear.output = FALSE\n  )\n\n# Gerando gráfico da rede\nplot(\n  net4neur,\n  rep = \"best\",\n  information = FALSE\n)\n\n\n\n\n\n\n\n\nPor fim, veja um exemplo gráfico de uma deep neural network com 2 hidden layers, sendo que a primeira possui 5 neurônios e a segunda 3.\n\n# Seed para replicação\nset.seed(123)\n\n# Criando modelo com 2 hidden layers (5 e 3 neurônios)\nnet2hidden &lt;- \n  neuralnet(\n    turnover ~ talento + sem.gest.conseq,\n    binary.data,\n    hidden = c(5, 3),\n    err.fct = \"ce\",\n    linear.output = FALSE\n  ) \n\n# Gerando gráfico da rede\nplot(\n  net2hidden,\n  rep = \"best\",\n  information = FALSE\n)\n\n\n\n\n\n\n\n\nO gráfico acima demonstra como é possível construir uma rede neural bastante complexa e de difícil interpretação, mesmo sobre um conjunto de dados simples. Vale ressaltar que o ponto mais relevante ao aplicar redes neurais não é obter redes complexas, mas sim um melhor o resultado das predições.\nA definição da quantidade de hidden layers ou os seus tamanhos não possuem regras gerais de configuração. Basicamente, a melhor configuração será encontrada por experimentação, utilizando diversas configurações e comparando para as métricas de cada modelo. Por isso que o processo de grid search é muito importante para a utilização de redes neurais. Uma vez que as possibilidades de configurações são infinitas (quantidade de camadas e número de neurônios para cada uma), normalmente é feito um levantamento aleatório de inúmeras possibilidades de configurações.\n\n\n7.5.1 Funções de Ativação\nAté esta parte do documento, os exemplos foram apresentados utilizando função de ativação logística. Porém, a função de ativação mais utilizada em deep learning chama-se ReLU (Rectified Linear Unit), ou simplesmente Rectifier. Para ser possível explicar a função Rectifier é preciso explicar primeiramente a função Tanh, que é base para a Rectifier, bem como as razões pelas quais não é recomendado utliizar a função logística em deep learning.\nAntes de entrar em detalhes das funções de ativação é importante ressaltar que os modelos de deep learning, especificamente do framework H2O (utilizado neste projeto), são treinados com gradient descent usando back propagation (mecanismo que atualiza os pesos de uma rede neural com base em alguma otimização, que no caso é gradient descent). Dito isto, voltamos às funções.\nA função logística tem uma curva na forma de “S” e conta com a seguinte equação:\n\\(f(z) = \\frac{1}{1+e^{-z}}\\), que vai de 0 a 1, e já exemplificada acima.\nAlguns dos problemas existentes nas redes neurais com ativação logística são:\n\nSaturação: um neurônio torna-se saturado quando sua saída atinge seu máximo ou seu mínimo. Um neurônio saturado prejudica o processo de otimização na back-propagation, pois o gradiente dos pesos desaparece ou chega muito próximo a zero.\nNão é centrada em zero: é uma função que não possui massa igual nos dois lados do zero. No caso da função logística isso ocorre por ter limites entre 0 e 1, fazendo com que os outputs sejam sempre positivos. Essa característica prejudica bastante o processo de otimização por gradient descent, pois faz com que a atualização dos pesos “ande em apenas uma direção”, e não em todas as direções possíveis. A otimização fica muito mais difícil.\nO parâmetro \\(e^{-x}\\) exige bastante poder computacional e faz com que a convergência seja mais lenta.\n\nA função de ativação Tanh (tangente hiperbólica) possui uma curva também em S, mas com limites de output entre -1 e 1. Dessa forma, sua massa é igualmente distribuída ao longo do ponto zero, tornando-a uma função centrada em zero. Isto já resolve um dos problemas da função logística apresentado, pois o processo de otimização fica mais fácil de ser executado. A função pode ser encarada como uma “normalização” da função sigmoide, sendo escrita da seguinte forma:\n\\[ f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\] Alguns dos problemas existentes nas redes neurais com ativação tanh são:\n\nSaturação dos neurônios (devido aos limites de -1 e 1), o que impacta na otimização por gradient descent (problema chamado de vanishing gradient).\nÉ exigido alto poder computacional nos parâmetros \\(e^x\\).\n\nA função ReLu (Rectified Linear Unit, ou Rectifier) é de simples entendimento: ela é linear para todos os valores positivos e zero para todos os valores negativos. Por ser zero para os valores negativos, significa que nem todos neurônios são ativados o tempo todo, o que faz com que o processamento seja consideravelmente menor. Por não ter limite na saída, o problema de saturação é resolvido. Ela é amplamente utilizada em deep learning, sendo expressa da seguinte forma:\n\\[\n    f(z)= max(0,z)\n\\]\nUm dos problemas da função ReLu pode ocorrer quando, por exemplo, um bias (uma constante) é inicializada na rede com um valor negativo muito grande, de tal forma que a soma dos inputs fica menor ou muito próxima a zero. Neste caso o neurônio é desabilitado logo na ativação. Porém, a solução pode ser simplesmente começar a inicialização dos bias sempre com valores positivos (critério que já é default no H2O, framework de machine leanring utilizado neste projeto).\nOutro problema que ocorre tanto na função logística quando na tanh é que a derivada (usada na otimização no processo de backpropagation) pode ser ou muito grande ou muito pequena, dependendo da característica do dado. Já na função ReLU este ponto também é resolvido, uma vez que a derivada da função será 1 para \\(z &gt; 0\\) e 0 para \\(z \\leq 0\\).\nHá ainda uma função chamada Leaky ReLu, uma variação da função ReLu, que “suaviza” os valores negativos, mas não os trata como zero. Isto facilita o processo de otimização. Mas esta ainda não é uma função muito utilizada na prática. Sua equação é \\(f(z)= max(0.01z,z)\\).\nVeja a seguir os gráficos dos outputs de cada uma das funções de ativação mencionadas para uma amostra simulada de 1000 observações (linhas de dados), com valores aleatoriamente gerados entre -10 e +10. Nos gráficos é possível perceber como as funções logística e tanh saturam nos extremos. É possível perceber também, ao passar o mouse sobre o gráfico da Leaky ReLu, como os valores negativos são diferentes de zero, embora muito próximos.\nResumindo, a função mais comum em termos de uso e de implementação nos frameworks de deep learning é a função ReLu, pois seu processamento é rápido e sua performance tem se demonstrado melhor que as funções logística e tanh.\nVeja mais sobre as funções de ativação neste link.\n\n\n7.5.2 Termos Úteis para Setup de Deep Neural Networks\nAlém da função de ativação, existem diversos parâmetros de setup que precisam ser definidos para rodar um deep learning. A maioria dos frameworks já possuem parâmetros default que atendem a maior parte dos casos. Porém, para casos em que há necessidade de ajustes (por exemplo, pelo tempo excessivo de processamento), seguem as descrições dos principais parâmetros:\n\nBackpropagation. Técnica que objetiva reduzir o erro geral de uma rede neural por meio de ajustes nos pesos. Para isso utiliza a regra da cadeia para calcular as derivadas de funções compostas. Se reduzirmos iterativamente o erro de cada peso, teremos uma série de pesos para produzir boas previsões. Ver detalhes.\nEpoch: uma época (epoch) é quando todo o conjunto de dados é percorrido pela rede neural por uma vez. É preciso múltiplas épocas em um modelo de deep learning porque utilizamos gradient descent para otimizar o processo de aprendizagem, que é um processo iterativo e exige várias “passagens” pelos dados para atualizar os pesos.\nBatch: é o tamanho máximo de dados que será processado por vez, é um lote.\nIterações: é o número de batchs necessários para completar uma época (epoch).\n\n\n\n7.5.3 Exemplo de Redes Neurais. Lendo o conjunto de dados do Titanic\nVamos buscar o arquivo csv do repositório. Após iremos excluir as variáveis a não serem utilizadas e separar o dataset em treino e teste.\n\n# Caso de estar sem internet e precisar puxar o dado localmente\n# df_titanic &lt;- read_csv(\"data/titanic.csv\") %&gt;%\n#   select(passengerid, survived, pclass, sex, age, sibsp, parch, fare) %&gt;% \n#   mutate_if(is.character, as.factor) %&gt;% \n#   dplyr::mutate(survived = as.factor(survived)) %&gt;% \n#   na.omit\n  \ndf_titanic &lt;-\n  read_csv(\"https://gitlab.com/dados/open/raw/master/titanic.csv\") %&gt;%\n  select(passengerid, survived, pclass, sex, age, sibsp, parch, fare) %&gt;%\n  mutate_if(is.character, as.factor) %&gt;%\n  dplyr::mutate(survived = as.factor(survived)) %&gt;%\n  na.omit \n\nRows: 1309 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, sex, ticket, cabin, embarked\ndbl (7): passengerid, survived, pclass, age, sibsp, parch, fare\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_titanic %&gt;% head(5) \n\n# A tibble: 5 × 8\n  passengerid survived pclass sex      age sibsp parch  fare\n        &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1           1 0             3 male      22     1     0  7.25\n2           2 1             1 female    38     1     0 71.3 \n3           3 1             3 female    26     0     0  7.92\n4           4 1             1 female    35     1     0 53.1 \n5           5 0             3 male      35     0     0  8.05\n\n\nNa sequência segue o dicionário dos dados:\n\nsurvived: sobreviveu \\(= 1\\); não sobreviveu \\(=0\\);\npclass: classe (1, 2 e 3, indicando primeira, segunda e terceira classe);\nsex: masculino (male) e feminino (female);\nage: idade do passageiro;\nsibsp: número de irmãos e cônjuge (siblings and spouse) a bordo;\nparch: número de pais e filhos (parents and children) a bordo;\nfare: tarifa paga pelo passageiro.\n\n\n7.5.3.1 Criando dummies (binarização) para fatores (variáveis categóricas)\nOs algortimos de redes neurais não trabalham com fatores (variáveis categóricas) diretamente. O passo que é realizado para que o algoritmo consiga trabalhar é criar n variáveis binárias (chamadas de dummies). Teremos então que cada dummy representará um nível de uma variável categórica.\nPor exemplo, ao utilizarmos a variável sex dos dados do titanic, obteremos duas dummies: sex.male e sex.female.\nPara fins matemáticos, evitando viés do modelo, de formar geral não recomenda-se que um conjunto de dummies forme uma matriz de soma 1 para cada linha. Por isso, uma categoria chave é escolhida para cada variável categórica e excluída do modelo. Exemplo: as dummies sex.male e sex.female sempre somarão 1 em cada linha, sendo assim, optamos por desconsiderar a variável sex.female, uma vez que informação do gênero feminino já está inclusa no caso de sex.male = 0.\n\ndf_titanic &lt;-\n  df_titanic %&gt;% \n  cbind(as_tibble(model.matrix(~ sex - 1 , data = .))) # Criando dummies para a variável sex\n\n\n\n7.5.3.2 Divindo em treino e teste\nAgora sim, iremos dividir o conjunto de dados em treino e teste.\n\n## set the seed to make your partition reproducible\nset.seed(123)\n\n# Create training subset\ntrain &lt;- df_titanic %&gt;% sample_frac(.70)\n# Create test subset\ntest  &lt;- anti_join(df_titanic, train, by = 'passengerid')\n\n\n\n7.5.3.3 Rodando os modelos\n\nlibrary(neuralnet)\nset.seed(123)\n\n# Simples, 1 neurônio, mas com vários inputs\nt1 &lt;- Sys.time()\nnet &lt;- neuralnet(\n  survived ~ age + pclass + fare + sexmale + sibsp + parch ,\n  train,\n  hidden = 1,\n  threshold = .3, # aumentar o threshold reduz o tempo para rodar, mas compromete a acurácia do modelo.\n  err.fct = \"sse\",\n  linear.output = FALSE\n)\nt2 &lt;- Sys.time()\n\nplot(\n  net,\n  rep = \"best\",\n  information = FALSE\n)\n\n\n\n\n\n\n\n\nO tempo de processamento foi:\n\nt2-t1\n\nTime difference of 0.0305438 secs\n\n\nVeja que o pacote neuralnet nos permite também aumentar o número de neurônios e de hidden layers. Veja como fica:\n\n# Modelo complexo, com duas hidden layers e diversos neurônios\nt3 &lt;- Sys.time()\nnet2 &lt;- neuralnet(\n  survived ~ age + pclass + fare + sexmale + sibsp + parch ,\n  data = train,\n  hidden = c(7,3), # número de neurônios a utilizar na hidden layer.\n  #stepmax=1e6, # dar mais tempo ao algoritmo para convergir, mas pode demorar horas!\n  threshold = .3, # aumentar o threshold reduz o tempo para rodar, mas compromete a acurácia do modelo.\n  err.fct = \"sse\", # usar \"sse\" ou \"ce\" para cálculo dos erros.\n  linear.output = FALSE\n)\nt4 &lt;- Sys.time()\n\nplot(\n  net2,\n  rep = \"best\",\n  information = FALSE\n)\n\n\n\n\n\n\n\n\nO tempo de processamento foi:\n\nt4-t3\n\nTime difference of 6.898422 secs\n\n\n\n\n\n7.5.4 Avaliação dos modelos\n\nModelo 1 - Rede neural com apenas um neurônio na hidden layer\nAplicando a função preditc().\n\npred1 &lt;- predict(net,\n                newdata =\n                  test %&gt;%\n                  select(survived, age, pclass, fare, sexmale, sibsp, parch)\n                )[, 2] # segunda coluna para pegar apenas survived = 1\n\nVeja a curva ROC do modelo estimado.\n\nlibrary(pROC)\n\npred1_roc &lt;- \n  data.frame(pred1, as.factor(as.numeric(test$survived)-1)) %&gt;% arrange(desc(pred1))\ncolnames(pred1_roc) &lt;- c(\"pred\", \"survived\")\n\nroc1 &lt;- roc(pred1_roc$survived , pred1_roc$pred, percent = TRUE)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\npar(pty = \"s\")\nplot.roc(\n  roc1,\n  print.auc = TRUE,\n  legacy.axes = TRUE,\n  xlab = \"Taxa de Falso Positivo (100 - Especificidade)\",\n  ylab = \"Taxa de Verdadeiro Positivo (Sensibilidade)\"\n)\n\n\n\n\n\n\n\n\n\n\n\n7.5.5 Modelo 2 - Rede neural com diversos neurônios e 2 hidden layers\nAplicando a função preditc().\n\npred2 &lt;- predict(net2,\n                newdata =\n                  test %&gt;%\n                  select(survived, age, pclass, fare, sexmale, sibsp, parch)\n                )[, 2] # segunda coluna para pegar apenas survived = 1\n\nVeja a curva ROC do modelo estimado.\n\nlibrary(pROC)\n\npred2_roc &lt;- \n  data.frame(pred2, as.factor(as.numeric(test$survived)-1)) %&gt;% arrange(desc(pred2))\ncolnames(pred2_roc) &lt;- c(\"pred\", \"survived\")\n\nroc2 &lt;- roc(pred2_roc$survived , pred2_roc$pred, percent = TRUE)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\npar(pty = \"s\")\nplot.roc(\n  roc2,\n  print.auc = TRUE,\n  legacy.axes = TRUE,\n  xlab = \"% de Falso Positivo (100 - Especificidade)\",\n  ylab = \"% de Verdadeiro Positivo (Sensibilidade)\"\n)\n\n\n\n\n\n\n\n\n\n\n7.5.6 Comparando a curva ROC dos dois modelos\n\npar(pty = \"s\")\nplot(roc1, print.auc = TRUE, col = \"blue\", main = \"ROC\", legacy.axes = TRUE, \n     xlab = \"% de Falso Positivo (100 - Especificidade)\",\n     ylab = \"% de Verdadeiro Positivo (Sensibilidade)\")\nplot(roc2, print.auc = TRUE, col = \"green\", print.auc.y = 40, add = TRUE, legacy.axes = TRUE)\nlegend(\"bottomright\", legend=c(\"RNA 1 Neur. e 1 H. Layer\", \"Deep Learning (+ de 1 H. Layer)\"),\n       col= c(\"blue\", \"green\"), lwd=2)\n\n\n\n\n\n\n\n\nDados dois modelos e as duas curvas ROC, será que a AUC dos dois modelos são estatisticamente diferentes? Para responder esta pergunta pode-se utilizar o teste de DeLong para duas curvas ROC. Veja o teste a seguir:\n\n# Rodando o teste DeLong, se p-valor &lt; 0.10, então há diferença, caso contrário não há.\nroc.test(roc1, roc2, method = \"delong\")\n\n\n    DeLong's test for two ROC curves\n\ndata:  roc1 and roc2\nD = -0.30642, df = 623.97, p-value = 0.7594\nalternative hypothesis: true difference in AUC is not equal to 0\nsample estimates:\nAUC of roc1 AUC of roc2 \n   84.52925    85.60029 \n\n\n\n\n7.5.7 Considerações\nEntre os modelos treinados, nem sempre é nítida a diferença entre o melhor apenas observando a curva ROC. Sendo assim, o teste de DeLong permite terum parâmetro objetivo para ajudar na escolha do melhor modelo. O teste possui a hipótese nula (\\(H_0\\)) de que a AUC dos testes são iguais.\nVeja também que nem sempre o aumento do porder computacional exigido traz resultados proporcionalmente melhores!\nO grande trade-off das redes neurais (e principalmente das deep learnings, que usam diversas hidden layers) é justamente este: até que ponto a necessidade exponencial de maior poder computacional gera de fato um ganho preditivo no modelo que está sendo treinado?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#avaliação-de-modelos-supervisionados",
    "href": "machine_learning.html#avaliação-de-modelos-supervisionados",
    "title": "5  Machine Learning para negócios",
    "section": "7.6 Avaliação de modelos supervisionados",
    "text": "7.6 Avaliação de modelos supervisionados\nÉ comum que na prática sejam aplicados modelos com variadas parametrizações e também diferentes famílias de modelos sobre um mesmo conjunto de dados de treino.\nCom isso, o tema avaliação de modelos de machine learning possui uma grande relevância, além de possuir temas bastante complexos e ser um tema extenso.\nA seguir alguns dos pontos principais são elencados acerca deste tópico.\n\nAvaliação de modelos de Classificação\nO caso mais clássico e certamente mais utilizado de classificador é o classificador binário: em uma tentativa, ou acerta ou erra.\nCom base neste tipo de classificador é possível construir uma grande quantidade de métricas de avaliação (algumas das quais podem inclusive serem generalizadas para classificadores com mais de duas classes).\nAlgumas principais destas métricas serão abordadas a seguir. Veremos:\n\nAcurácia;\nPrecisão;\nTabela (ou matriz) confusão;\nTaxas criadas com a matriz confusão;\nCurva ROC;\nÁrea soba curva ROC\n\nNota: Apesar de parecer básico, a Wikipedia possui uma rede de conteúdos de alta qualidade sobre os temas que cercam a matriz confusão. Veja.\nAntes de entrar nas métricas, vale lembrar que é comum que na prática os analistas se refiram a assertividade de um classificador apenas pelo percentual de acertos (acurácia), ou ainda pelo percentual de casos positivos que o modelo de fato acertou (precisão). Veja:\nAcurácia é o percentual de acertos sobre todas as apostas do algoritmo.\n\\[ Acurácia = \\frac{Acertos}{Total} \\]\nA acurácia, porém, possui diversas limitações. Em alguns casos, o conceito de precisão pode se encaixar melhor. Veja um exemplo:\n\nExemplo. Em uma campanha de marketing, a empresa deseja enviar seus materiais apenas para os indivíduos que um modelo preditivo afirma que irão comprar o produto (a variável resposta ou dependente é binária, indicando a compra ou não compra do produto). Essa abordagem reduz muito os custos quando feita um comparação com o envio dos materiais publicitários para toda a base de indivíduos que a empresa possui.\n\nNeste caso, olhar apenas para o percentual de preditos positivos corretos dentro de todos casos preditos positivos (precisão) é a melhor saída para escolher o melhor modelo de machine learning.\nPrecisão é o percentual dos casos que o algoritmo classificou como positivo e acertou sobre todas as respostas que foram preditas como positivas.\n\\[ Precisão = \\frac{\\text{Respostas postivas corretas}}{\\text{Total de positivos preditos}} \\]\nPorém, apenas a acurácia e a precisão não são suficientes.\nPara entender melhor este tema é preciso incialmente compreender os tipos de erros e acertos possíveis em classificadores.\nTipos de erros e acertos em modelos de classificação:\n\nVerdadeiro Positivo (VP): falou que seria, e foi.\nVerdadeiro Negativo (VN): falou que não seria, e não foi.\nFalso Positivo (FP): falou que seria, mas não foi.\nFalso Negativo (FN): falou que não seria, mas foi.\n\nTabela (ou matriz) confusão (confusion matrix)\n\n\n\n\n\n\n\n\n\nReal  Positivo = 1\nReal  Negativo = 0\n\n\n\n\nPredito  Positivo = 1 \nVP\nFP\n\n\nPredito  Negativo = 0 \nFN\nVN\n\n\n\nPara refletir …\n\nOs custos atrelados a cada tipo de erro possível são os mesmos? Ou ainda, os benefícios associados a cada tipo de acerto possuem o mesmo peso?\n\nPara responder as perguntas acima é preciso olhar para as taxas que podem ser extraídas da matriz confusão. Estas taxas resumem a capacidade de um modelo de acertar suas predições, cada uma apresentando um olhar diferente. Veja:\n\n\n\n\n\n\n\n\nMétrica\nFórmula\nEx.\n\n\n\n\nTaxa de Verdadeiro Positivo (TVP), Sensibilidade ou Recall\nTVP = VP / (FN+VP)\n72 %\n\n\nTaxa de Falso Positivo (TFP, ou 1 - Especificidade)\nTFP = FP / (VN+FP)\n4 %\n\n\nTaxa de Verdadeiro Negativo (TVN) ou Especificidade\nTVN = VN / (VN+FP)\n96 %\n\n\nTaxa de Falso Negativo (TFN)\nTFN = FN / (FN+VP)\n28 %\n\n\nAcurácia\nAcc = (VP+VN) / (VP+VN+FP+FN)\n85 %\n\n\nPrecisão\nPrec = VP / (VP+FP)\n93 %\n\n\n\nResumidamente, cada uma delas diz o seguinte:\n\nTaxa de Verdadeiro Positivo (TVP): percentual que foi predito positivo corretamente sobre o total que de fato era positivo.\nTaxa de Falso Positivo (TFP): percentual que foi predito positivo sobre o total que de fato era negativo.\nTaxa de Verdadeiro Negativo (TVN): percentual que foi predito negativo corretamente sobre o total que de fato era negativo.\nTaxa de Falso Negativo (TFN): percentual que foi predito negativo sobre o total que de fato era positivo.\n\n\nCurva ROC (Receiver Operating Characteristic)\nVeja que um modelo de classificação binária geralmente fornece uma probabilidade da variável target ser \\(=1\\), e não uma resposta fixa de 0 ou 1.\nSendo assim, é preciso definir um threshold (limite, ou ponto de corte), a qual definirá se a predição será considerada como 0 ou 1 (ocorrência ou não ocorrência de um evento). Podemos também olhar estes valores em percentual, de 0 a 100%.\nOu seja, dependendo do threshold escolhido, o mesmo modelo pode apresentar uma ótima ou uma péssima predição. Neste sentido, as curvas ROC ajudam a identificar o melhor modelo dado n thresholds.\nPara construir a Curva ROC é preciso:\n\nelencar os dados em ordem decrescente da probabilidade estimada pelo modelo;\ncalcular a taxa de verdadeiro positivo (será o eixo y da curva) e a taxa de falso positivo (que será o eixo x da curva) para cada linha (sempre considerando de forma cumulativa).\n\nVeja alguns exemplos de como ficam as curvas ROC dos modelos a seguir:\n\nModelo perfeito: a medida que o threshold vai diminuindo o modelo nunca comete um falso positivo e a taxa de verdadeiro positivo está sempre em 100%.\n\n\n\nModelo usual: comete alguns falsos positivos e falsos negativos.\n\n\n\nModelo que não agrega: um modelo que não agrega nada a mais em relação a chutes aleatórios. \nModelo totalmente errado: faz todas classificações de forma errada. É pior que o chute aleatório. \n\nVeja exemplo de construção da curva ROC com o Excel, a ser apresentado.\nVeja também o exemplo dos dados que a função roc() do R utiliza para criar as curvas.\n\nroc_df &lt;-\n  dplyr::tibble(\n    TVP = roc1$sensitivities,\n    TFP = (100 - roc1$specificities),\n    Thresholds = roc1$thresholds\n  )\n\nDT::datatable(\n  roc_df %&gt;% arrange(desc(Thresholds))\n)\n\n\n\n\n\nAlguns pontos:\n\nQuando o threshold é 1, então todos os valores serão preditos como 0, o que faz com que não haja nenhuma verdadeiro positivo e nenhum falso positivo. O que torna a TVP = 0 e a TFP = 0.\nA medida em que o threshold é reduzido, começam a surgir os casos de verdadeiro positivo e falso positivo, aumentando as TVP e TFP.\nA ideia é que os verdadeiros positivos (VP) sejam clasificados em um número maior possível a medida que o threshold é reduzido, antes que os falsos positivos comecem a aumentar muito.\nO threshold ótimo pode ser escolhido pelo ponto onde a variação do crescimento do crescimento da curva ROC começa a cair. Em outras palavras, no ponto em que temos a maior possível TVP para a menor possível TFP.\nO modelo utópico perfeito classifica todos os verdadeiros positivos corretamente sem nenhum falso positivo, tendo TVP = 100% e TFP = 0%.\nSe o threshold for 0, então todos valores serão preditos como 1, o que faz com que todos os verdadeiros positivos sejam classificados corretamente (TVP = 100%) e nenhum verdadeiro negativo seja classificado. Como não há verdadeiro negativo, então a Taxa de Falso Positivo (TFP) é 100% também.\n\n\n\n\nAvaliação de modelos de Regressão\nNos modelos de regressão, em uma tentativa, não há certo ou errado, mas sim o quão próximo chegou. Portanto, avalia-se o modelo pelo erro do predicted vs actual na lógica do quanto menor, melhor.\nO que veremos:\n\nMAE e RMSE (medidas de erro) - Ver mais\n\\(R^2\\) mede o percentual que as variáveis independentes (\\(X\\)) explicam a dependente (\\(y\\)).\n\nMAE (Mean Absolute Error) representa a média dos erros em conjunto de predições.\n\\[ MAE = \\frac{1}{n} \\sum^n_{j=1} | y_j - \\hat{y}_j | \\]\nRMSE (Root Mean Squared Error) representa a raiz da média dos quadrados dos erros. A equação é igual a do desvio-padrão, porém, ao invés de fazer a diferença de cada observação com a média, a faz contra os valores preditos.\nPor utilizar os quadrados dos erros, valores grandes são penalizados, elevando o erro e piorando a avaliação do modelo.\n\\[ RMSE = \\sqrt{ \\frac{1}{n} \\sum^n_{j=1} ( y_j - \\hat{y}_j )^2 } \\]\nR-quadrado (ou coeficiente de determinação)\nMostra o quão bem as variáveis independentes (explicativas) explicam a variabilidade da variável dependente (resposta).\nVai de 0 a 1 (ou em percentual, de 0% a 100%), indicando a proporção da variabilidade da variável a ser predita que é explicada pelas variáveis que constam no modelo.\nR-quadrado Ajustado\nMesmo conceito do R-quadrado, mas faz ajustes em relação ao número de variáveis do modelo.\nO R-quadrado Ajustado leva em conta a melhoria marginal de adicionar uma nova variável ao modelo.\nSempre será igual ou menor que o R-quadrado, e destas duas é a métrica preferível.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>*Machine Learning* para negócios</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Provost, F., T. Fawcett, and M. Boscato. 2016. Data Science Para\nNegócios. ALTA BOOKS. https://books.google.com.br/books?id=c4lAvgAACAAJ.",
    "crumbs": [
      "References"
    ]
  }
]